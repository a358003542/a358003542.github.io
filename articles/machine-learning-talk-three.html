<!DOCTYPE html>
<html lang="zh-cn">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="baidu-site-verification" content="D4VqC4HppC"/>
    <meta name="msvalidate.01" content="55CB117A61A6F8286173763FB18D9625"/>

        <meta name="author" content="cdwanze"/>
        <meta name="copyright" content="cdwanze"/>

        <meta property="og:type" content="article"/>
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="machine-learning, 机器学习, " />

<meta property="og:title" content="机器学习第三谈 "/>
<meta property="og:url" content="/articles/machine-learning-talk-three.html" />
<meta property="og:description" content="前言 在上一文机器学习第二谈中，我们算是搭建了一个简单的神经网络，初步了解了一下大体情况。本文主要紧跟着对相关内容进行更细致的讨论。 relu激活函数 relu激活函数具体的数学运算公式很简单，就是： z = np.maximum(z, 0) 上面运算就是对z张量进行了relu运算了，按元素的，如果元素值大于0则为原元素的值，否则为0。 广播(broadcasting) 广播一种操作，shape较小的张量和shape较大的张量进行点对点运算时，需要对shape较小的张量进行广播操作，使其在运算上shape兼容。 广播具体操作规则是： shape较小的张量添加新的维度是的两个张量维度数相同 shape较小的张量在新的维度中的数据是重复的，相当于没有原维度的数据，即： y[1,j] = y[2,j] = y[3,j] =... y[j] 张量点积 矩阵乘法也就是这里的张量点积学过线性代数的对这个概念还是很清楚了，不过到更高的维度的张量的点积情况似乎有点复杂了。这里我们需要把张量点积的shape变化弄清楚，后面可能会有用的，具体实际张量运算可以交给函数去做。 $$ x \cdot y = z $$ x …" />
<meta property="og:site_name" content="cdwanze的博文" />
<meta property="og:article:author" content="cdwanze" />
<meta property="og:article:published_time" content="2018-11-06T00:00:00+08:00" />
<meta property="" content="2018-11-06T00:00:00+08:00" />
<meta name="twitter:title" content="机器学习第三谈 ">
<meta name="twitter:description" content="前言 在上一文机器学习第二谈中，我们算是搭建了一个简单的神经网络，初步了解了一下大体情况。本文主要紧跟着对相关内容进行更细致的讨论。 relu激活函数 relu激活函数具体的数学运算公式很简单，就是： z = np.maximum(z, 0) 上面运算就是对z张量进行了relu运算了，按元素的，如果元素值大于0则为原元素的值，否则为0。 广播(broadcasting) 广播一种操作，shape较小的张量和shape较大的张量进行点对点运算时，需要对shape较小的张量进行广播操作，使其在运算上shape兼容。 广播具体操作规则是： shape较小的张量添加新的维度是的两个张量维度数相同 shape较小的张量在新的维度中的数据是重复的，相当于没有原维度的数据，即： y[1,j] = y[2,j] = y[3,j] =... y[j] 张量点积 矩阵乘法也就是这里的张量点积学过线性代数的对这个概念还是很清楚了，不过到更高的维度的张量的点积情况似乎有点复杂了。这里我们需要把张量点积的shape变化弄清楚，后面可能会有用的，具体实际张量运算可以交给函数去做。 $$ x \cdot y = z $$ x …">

    <title>
机器学习第三谈  · cdwanze的博文
</title>


        <link href="/theme/css/font-awesome.css" rel="stylesheet"
              media="screen">
        <link href="/theme/css/bootstrap.min.css" rel="stylesheet"
              media="screen">

            <link rel="stylesheet" type="text/css"
                  href="/theme/css/pygments.css" media="screen">
            <link rel="stylesheet" type="text/css"
                  href="/theme/css/elegant.css" media="screen">
            <link rel="stylesheet" type="text/css"
                  href="/theme/css/custom.css" media="screen">






</head>
<body>

<nav class="navbar">
    <div class="navbar navbar-default" role="navigation">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed"
                    data-toggle="collapse" data-target=".navbar-collapse"
                    aria-expanded="false">
                <span class="sr-only">Navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>

            <a class="navbar-brand" href="https://www.cdwanze.work"><span
                    class=site-name>网站首页</span></a>
        </div>


        <div class="navbar-collapse collapse">
            <form action="/search.html"
                  onsubmit="return validateForm(this.elements['q'].value);"
                  class="navbar-form navbar-left">
                <div class="form-group">
                    <input type="text" name="q" id="tipue_search_input"
                           class="form-control" placeholder="Search..."
                           style="width:430px;">
                </div>
                <button class="btn btn-default" type="submit">搜索</button>
            </form>


            <ul class="nav navbar-nav nav-pills navbar-right">
                <li >
                    <a href="">博文首页</a></li>

                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle"
                           data-toggle="dropdown" role="button"
                           aria-haspopup="true" aria-expanded="false">查找文章<span
                                class="caret"></span></a>
                        <ul class="dropdown-menu">
                            <li><a class="slowscroll" href="/categories.html">按分类</a>
                            </li>
                            <li><a class="slowscroll" href="/tags.html">按标签</a>
                            </li>
                        </ul>
                    </li>


                        <li >
                            <a href="/guan-yu-ben-wang-zhan.html">关于本网站</a>
                        </li>
                        <li >
                            <a href="/gong-gao.html">公告</a>
                        </li>
            </ul>


        </div>
    </div>
</nav>


<div class="container-fluid">
    <div class="col-md-1 col-md-1-left"></div>
    <div class="col-md-10">
<article>
<div class="row">
    <header class="page-header col-md-10 col-md-offset-2">
    <h1><a href="/articles/machine-learning-talk-three.html"> 机器学习第三谈  </a></h1>
    </header>
</div>

<div class="row">
    <div class="col-md-2 table-of-content">
        <nav>
        <h4>目录</h4>
        <div class="toc">
<ul>
<li><a href="#_1">前言</a></li>
<li><a href="#relu">relu激活函数</a></li>
<li><a href="#broadcasting">广播(broadcasting)</a></li>
<li><a href="#_2">张量点积</a></li>
<li><a href="#_3">张量变形</a></li>
<li><a href="#_4">张量的导数</a></li>
<li><a href="#sgd">随机梯度下降(SGD)</a></li>
<li><a href="#_5">二分类问题</a><ul>
<li><a href="#_6">准备数据</a></li>
<li><a href="#_7">验证集</a></li>
<li><a href="#history">history作图</a></li>
<li><a href="#_8">总结</a></li>
</ul>
</li>
<li><a href="#_9">多分类问题</a></li>
<li><a href="#_10">回归问题</a><ul>
<li><a href="#_11">取值范围差异很大的数据</a></li>
<li><a href="#k">K折验证</a></li>
<li><a href="#_12">总结</a></li>
</ul>
</li>
<li><a href="#_13">参考资料</a></li>
</ul>
</div>
        </nav>
    </div>
    <div class="col-md-8 article-content">

            
            
<h2 id="_1">前言</h2>
<p>在上一文机器学习第二谈中，我们算是搭建了一个简单的神经网络，初步了解了一下大体情况。本文主要紧跟着对相关内容进行更细致的讨论。</p>
<h2 id="relu">relu激活函数</h2>
<p>relu激活函数具体的数学运算公式很简单，就是：</p>
<div class="highlight"><pre><span></span>z = np.maximum(z, 0)
</pre></div>
<p>上面运算就是对z张量进行了relu运算了，按元素的，如果元素值大于0则为原元素的值，否则为0。</p>
<h2 id="broadcasting">广播(broadcasting)</h2>
<p>广播一种操作，shape较小的张量和shape较大的张量进行点对点运算时，需要对shape较小的张量进行广播操作，使其在运算上shape兼容。</p>
<p>广播具体操作规则是：</p>
<ul>
<li>
<p>shape较小的张量添加新的维度是的两个张量维度数相同</p>
</li>
<li>
<p>shape较小的张量在新的维度中的数据是重复的，相当于没有原维度的数据，即： y[1,j] = y[2,j] = y[3,j] =... y[j]</p>
</li>
</ul>
<h2 id="_2">张量点积</h2>
<p>矩阵乘法也就是这里的张量点积学过线性代数的对这个概念还是很清楚了，不过到更高的维度的张量的点积情况似乎有点复杂了。这里我们需要把张量点积的shape变化弄清楚，后面可能会有用的，具体实际张量运算可以交给函数去做。</p>
<div class="math">$$
x \cdot y = z
$$</div>
<p>
x shape (a, b) y shape (b,c) 输出 z的 shape(a, c) </p>
<p>高维的情况如下：</p>
<p>(a, b, c ,d) · (d,) -&gt; (a,b,c)</p>
<p>(a, b, c ,d) · (d, e) -&gt; (a,b,c, e)</p>
<h2 id="_3">张量变形</h2>
<p>ndarray可以直接调用reshape方法来进行张量变形操作，变形后元素总个数应该不变，也就是各个维度容量乘积数不变。</p>
<h2 id="_4">张量的导数</h2>
<p>张量的导数叫做梯度。</p>
<h2 id="sgd">随机梯度下降(SGD)</h2>
<p>小批量SGD过程如下：</p>
<ol>
<li>抽取训练样本x和对应目标y组成数据批量</li>
<li>在x上运行网络，得到预测值y_pred</li>
<li>计算网络在这批数据上的损失，用于衡量y_pred和y之间的距离</li>
<li>计算损失相对于网络参数的梯度</li>
<li>将参数沿着梯度的反方向移动一点 W -= step * gradient ，从而使这批数据上的损失减小一点。这里的step即步长也叫学习率。</li>
</ol>
<p>目前实践中的优化器optimizer都采用的是随机梯度下降，不同的是各自进行了某些优化，这些SGD变体有：带动量的SGD，Adagrad，RMSProp等。</p>
<h2 id="_5">二分类问题</h2>
<p>imdb电影评论二分类问题，输出评论文字，然后得出评论积极还是消极。</p>
<h3 id="_6">准备数据</h3>
<p>imdb电影评论的数据，其内部建立了一个字典索引，某个整数对应某个单词。</p>
<p>具体某一个评论是一个整数序列，下面有两种方法将这个整数序列张量化：</p>
<ol>
<li>填充或截取，使得每个整数序列具有相同的长度，然后神经网络第一层必须是 Embedding 层。</li>
<li>对列表进行one-hot编码，比如说[3,5]在长度10的情况下编码为 [0,0,1,0,1,0,0,0,0,0] ，然后第一层使用Dense层，这个前面提到过了。</li>
</ol>
<h3 id="_7">验证集</h3>
<p>在model.fit 里面你可以通过 <code>validation_data</code> 参数来指定验证集，验证集和训练模型无关，是一个epoch之后来运算验证当前模型的效果，可以及时发现模型出现过拟合问题或者其他问题，从而决定是否终止训练模型。</p>
<p>你可以从训练集里面切一部分下来作为训练集，也可以直接使用测试集作为验证集【这里简单起见就直接用测试集做了验证集，正式的做法叫做留出验证集法，是应该操作如下：训练数据里面一部分作为训练集，剩下来的一部分作为验证集，然后用训练集训练，验证集评估当前模型的好坏。模型参数调节好训练好之后，记得最后用整个训练集从头训练一次，最后用另外的测试集数据测试下模型的实际效果】。</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.datasets</span> <span class="kn">import</span> <span class="n">imdb</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">),(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">)</span> <span class="o">=</span> <span class="n">imdb</span><span class="o">.</span><span class="n">load_data</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">asarray</span><span class="p">(</span><span class="n">train_labels</span><span class="p">)</span><span class="o">.</span><span class="kp">astype</span><span class="p">(</span><span class="s1">'float32'</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">asarray</span><span class="p">(</span><span class="n">test_labels</span><span class="p">)</span><span class="o">.</span><span class="kp">astype</span><span class="p">(</span><span class="s1">'float32'</span><span class="p">)</span>
</pre></div>
<div class="highlight"><pre><span></span>word_index = imdb.get_word_index()
reversed_word_index = dict([value,key] for key,value in word_index.items())
decoded_review = ' '.join([reversed_word_index.get(i-3, '?') for i in train_data[0]])
decoded_review
</pre></div>
<p>这里采用的是one-hot编码，值得注意的是一句话如果有几个重复的单词，将是被忽略的，第一种方案填充截取方案会保留词语顺序，这个在自然语言处理中是很重要的一个因素。</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="k">def</span> <span class="nf">verctorize_sequences</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="n">dimension</span><span class="o">=</span><span class="mi">10000</span><span class="p">):</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">),</span> <span class="n">dimension</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sequence</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sequences</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">sequence</span><span class="p">:</span>
            <span class="n">results</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">d</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">results</span>
</pre></div>
<div class="highlight"><pre><span></span>x_train = verctorize_sequences(train_data)
x_test = verctorize_sequences(test_data)
</pre></div>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span><span class="n">Dropout</span>
<span class="kn">from</span> <span class="nn">keras.optimizers</span> <span class="kn">import</span> <span class="n">RMSprop</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10000</span><span class="p">,)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'sigmoid'</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
<div class="highlight"><pre><span></span>model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])
</pre></div>
<div class="highlight"><pre><span></span>history = model.fit(x_train, y_train, epochs=4, batch_size=512, validation_data=(x_test, y_test))
</pre></div>
<h3 id="history">history作图</h3>
<p>keras出来的history并没提供作图函数，下面简单整理了一下：</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">plot_history_loss</span><span class="p">(</span><span class="n">history</span><span class="p">):</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">'acc'</span><span class="p">]</span>
    <span class="n">val_acc</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">'val_acc'</span><span class="p">]</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">'loss'</span><span class="p">]</span>
    <span class="n">val_loss</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">'val_loss'</span><span class="p">]</span>

    <span class="n">epochs</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">acc</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># "bo" is for "blue dot"</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">'bo'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Training loss'</span><span class="p">)</span>
    <span class="c1"># b is for "solid blue line"</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">,</span> <span class="s1">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Validation loss'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Training and validation loss'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Epochs'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Loss'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">plot_history_loss</span><span class="p">(</span><span class="n">history</span><span class="p">)</span>
</pre></div>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_history_acc</span><span class="p">(</span><span class="n">history</span><span class="p">):</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">'acc'</span><span class="p">]</span>
    <span class="n">val_acc</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">'val_acc'</span><span class="p">]</span>

    <span class="n">epochs</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">acc</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">acc</span><span class="p">,</span> <span class="s1">'bo'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Training acc'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_acc</span><span class="p">,</span> <span class="s1">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Validation acc'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Training and validation accuracy'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Epochs'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Loss'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">plot_history_acc</span><span class="p">(</span><span class="n">history</span><span class="p">)</span>
</pre></div>
<p>这个基本上ephch超过2,3之后就开始过拟合了，但是精度没有超过90%的。Keras中的example相关看了一下，就算使用CNN提升也不是很明显，倒是用了数据预处理这块使用了二元模型，准确度稍微提升了一点，可见就imdb这个例子，必须加上自然语言处理相关的数据预处理步骤才能更好地提升准确度。</p>
<h3 id="_8">总结</h3>
<ol>
<li>使用验证集和绘图能够很好地观察过拟合现象，这个不能省，最后测试集环节省了验证集也不要省。</li>
<li>二分类问题最后一层激活函数推荐 sigmoid，损失函数推荐 binary_crossentorpy</li>
<li>relu激活的Dense层堆叠，可以解决很多问题。</li>
<li>无论问题是什么，rmsprop优化器通常都是一个好的选择。</li>
</ol>
<h2 id="_9">多分类问题</h2>
<p>将某个数据点划分为某一个类别，但是有多个分类的问题是单标签多分类问题；如果某个数据点可以划分为多个分类，则为多标签多分类问题。</p>
<p>多分类问题在处理上和二分类问题很类似，除了一些细节上的差异：</p>
<ol>
<li>标签数据张量化可以使用 <code>to_categorical</code> 来进行one-hot编码，然后损失函数需要选择 <code>categorical_crossentropy</code> 。或者标签数据直接作为整数标签转成ndarray对象送入，这时需要选择损失函数 <code>sparse_categorical_crossentropy</code> ，这两者只是接口差异，内部算法一样的。</li>
<li>多分类问题神经网络最后一层应该是对应N个分类的N个units的Dense层。</li>
<li>多分类问题神经网络应该避免使用太小的中间层，以免出现信息瓶颈。</li>
</ol>
<h2 id="_10">回归问题</h2>
<p>回归问题预测是连续的值而不是离散的标签。</p>
<h3 id="_11">取值范围差异很大的数据</h3>
<p>取值范围差异很大的数据送入神经网络需要先进行标准化处理。这里指的标准化是正态分布那边的概念，也就是计算每个特征值的z-score标准分。即 每个特征值减去本特征的平均值然后除以本特征的标准差。</p>
<div class="highlight"><pre><span></span>mean = train_data.mean(axis=0)
</pre></div>
<p>这是计算维度0的均值，或者说是计算每个特征列的均值。减去操作如下：</p>
<div class="highlight"><pre><span></span>train_data -= mean
</pre></div>
<p>这里往细上将还进行了mean的广播操作，所以才能按照元素点对点的执行了减法操作。</p>
<p>这是计算每个特征列的标准差：</p>
<div class="highlight"><pre><span></span>std = train_data.std(axis=0)
</pre></div>
<p><strong>注意：</strong> 用于测试数据标准化的均值和标准差都是训练数据上的。在工作流程上，你不能从测试数据上计算得到任何结果。</p>
<h3 id="k">K折验证</h3>
<p>如果可用数据较少，可以使用K折验证来可靠地评估模型。</p>
<p>K折验证是将验证数据分成K分，重复K次，每次选取一个作为测试集，其他作为训练集。模型的最终验证分数等于K个验证分数的均值。</p>
<h3 id="_12">总结</h3>
<ol>
<li>回归问题神经网络最后一层没有激活函数，可以返回任意范围内的值。</li>
<li>回归问题常用损失函数：mse损失函数。MSE（mean squared error） 均方误差损失函数。</li>
<li>回归问题监控指标：平均绝对误差。MAS （mean absolute error）是预测值和目标值之差的绝对值。</li>
<li>如果可用的训练数据较少，最好使用隐藏层较少的小型网络，避免严重的过拟合</li>
</ol>
<h2 id="_13">参考资料</h2>
<ol>
<li>机器学习实战 Peter Harrington 著 李锐 李鹏等译</li>
<li><a href="http://ml.apachecn.org/mlia/">机器学习实战线上教程</a></li>
<li>python深度学习 弗朗索瓦·肖奈</li>
</ol>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','mhchem.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
            
            
            <hr/>

        </div>
        <section>
        <div class="col-md-2" style="float:right;font-size:0.9em;">
            <h4>首发于：</h4>
            <time pubdate="pubdate" datetime="2018-11-06T00:00:00+08:00">2018年 11月 6日 </time>

<h4>最近更新于：</h4>
<time datetime="2018-11-06T00:00:00+08:00">2018年 11月 6日 </time>

            <h4>分类：</h4>
            <a class="category-link" href="/categories.html#ji-qi-xue-xi-ref">机器学习</a>
            <h4>标签：</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="/tags.html#machine-learning-ref">machine-learning
                    <span>8</span>
</a></li>
            </ul>
        </div>
        </section>
</div>
</article>
    </div>
    <div class="col-md-1"></div>

</div>


<div id="push"></div>


<footer>
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-power">Powered by <a href="http://getpelican.com/" title="Pelican Home Page">Pelican</a>. Theme: <a href="http://oncrashreboot.com/pelican-elegant" title="Theme Elegant Home Page">Elegant</a> by <a href="http://oncrashreboot.com" title="Talha Mansoor Home Page">Talha Mansoor</a> and updated by <a href="http://www.cdwanze.work" title="cdwanze Home Page">cdwanze</a></li>
    </ul>
</div>
</footer>

        <script src="/theme/js/jquery.min.js"></script>
    <script src="/theme/js/bootstrap.min.js"></script>
    <script>
        function validateForm(query) {
            return (query.length > 0);
        }
    </script>

    <script>
        function adjust_search_width() {
            var w = document.documentElement.clientWidth;
            if ((w > 755) && (w < 975)) {
                plus_width = w - 755;
                $('.navbar-form .form-control').outerWidth(210 + plus_width);
            } else if (w >= 975) {
                $('.navbar-form .form-control').outerWidth(210 + 220);
            } else if (w <= 755) {
                $('.navbar-form .form-control').css('width', '100%')
            }
        }

        $(document).ready(function () {
            adjust_search_width();
        });

        window.onresize = function () {
            adjust_search_width();
        }

    </script>


    



</body>
</html>