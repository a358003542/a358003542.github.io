<!DOCTYPE html>
<html lang="zh-cn">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="baidu-site-verification" content="D4VqC4HppC"/>
    <meta name="msvalidate.01" content="55CB117A61A6F8286173763FB18D9625"/>

        <meta name="author" content="cdwanze"/>
        <meta name="copyright" content="cdwanze"/>

        <meta property="og:type" content="article"/>
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="python, 网络抓取, python好伙伴, " />

<meta property="og:title" content="scrapy模块 "/>
<meta property="og:url" content="https://docs.cdwanze.work/articles/scrapy-module.html" />
<meta property="og:description" content="简介 Scrapy模块和Django模块在使用上包括在内容目录结构上都很相似，当然这两个项目干的完全是两个不同的事情，我想这种相似性更多的可以描述为类似于大多数GUI界面的那种类似。 我之前尝试过写过一个小的Spider网络爬虫程序，其实网络爬取大体过程都是类似的，因此我学习Scrapy项目大体只是一些基本配置的了解，对于其内部原理已经很熟悉了，所以本文不会在这些地方赘述了。 scrapy就是一个python模块，可以通过 pip 安装之，所以安装这块也不多说了。 新建一个项目 scrapy startproject project_name [path] 因为我喜欢创建python的venv虚拟环境，所以上面最后path设置为 . 当前目录下的意思。 然后接下来就是编写爬虫Spider脚本还有等等其他一些配置了。 第一个例子 下面是一个简单的例子： class MySpider(scrapy.Spider): user_agent = get_random_user_agent() name = &#34;7yrt&#34; start_urls = [&#39;http://7yrt.com/html/rhrt/&#39;] def parse(self, response): url = response.url html = response.text ## do …" />
<meta property="og:site_name" content="cdwanze的博文" />
<meta property="og:article:author" content="cdwanze" />
<meta property="og:article:published_time" content="2017-10-11T00:00:00+08:00" />
<meta property="" content="2019-10-08T00:00:00+08:00" />
<meta name="twitter:title" content="scrapy模块 ">
<meta name="twitter:description" content="简介 Scrapy模块和Django模块在使用上包括在内容目录结构上都很相似，当然这两个项目干的完全是两个不同的事情，我想这种相似性更多的可以描述为类似于大多数GUI界面的那种类似。 我之前尝试过写过一个小的Spider网络爬虫程序，其实网络爬取大体过程都是类似的，因此我学习Scrapy项目大体只是一些基本配置的了解，对于其内部原理已经很熟悉了，所以本文不会在这些地方赘述了。 scrapy就是一个python模块，可以通过 pip 安装之，所以安装这块也不多说了。 新建一个项目 scrapy startproject project_name [path] 因为我喜欢创建python的venv虚拟环境，所以上面最后path设置为 . 当前目录下的意思。 然后接下来就是编写爬虫Spider脚本还有等等其他一些配置了。 第一个例子 下面是一个简单的例子： class MySpider(scrapy.Spider): user_agent = get_random_user_agent() name = &#34;7yrt&#34; start_urls = [&#39;http://7yrt.com/html/rhrt/&#39;] def parse(self, response): url = response.url html = response.text ## do …">

    <title>
scrapy模块  · cdwanze的博文
</title>


        <link href="https://docs.cdwanze.work/theme/css/font-awesome.css" rel="stylesheet"
              media="screen">
        <link href="https://docs.cdwanze.work/theme/css/bootstrap.min.css" rel="stylesheet"
              media="screen">

            <link rel="stylesheet" type="text/css"
                  href="https://docs.cdwanze.work/theme/css/pygments.css" media="screen">
            <link rel="stylesheet" type="text/css"
                  href="https://docs.cdwanze.work/theme/css/elegant.css" media="screen">
            <link rel="stylesheet" type="text/css"
                  href="https://docs.cdwanze.work/theme/css/custom.css" media="screen">






</head>
<body>

<nav class="navbar">
    <div class="navbar navbar-default" role="navigation">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed"
                    data-toggle="collapse" data-target=".navbar-collapse"
                    aria-expanded="false">
                <span class="sr-only">Navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>

            <a class="navbar-brand" href="https://www.cdwanze.work"><span
                    class=site-name>网站首页</span></a>
        </div>


        <div class="navbar-collapse collapse">
            <form action="https://docs.cdwanze.work/search.html"
                  onsubmit="return validateForm(this.elements['q'].value);"
                  class="navbar-form navbar-left">
                <div class="form-group">
                    <input type="text" name="q" id="tipue_search_input"
                           class="form-control" placeholder="Search..."
                           style="width:430px;">
                </div>
                <button class="btn btn-default" type="submit">搜索</button>
            </form>


            <ul class="nav navbar-nav nav-pills navbar-right">
                <li >
                    <a href="https://docs.cdwanze.work">博文首页</a></li>

                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle"
                           data-toggle="dropdown" role="button"
                           aria-haspopup="true" aria-expanded="false">查找文章<span
                                class="caret"></span></a>
                        <ul class="dropdown-menu">
                            <li><a class="slowscroll" href="/categories.html">按分类</a>
                            </li>
                            <li><a class="slowscroll" href="/tags.html">按标签</a>
                            </li>
                        </ul>
                    </li>


                        <li >
                            <a href="https://docs.cdwanze.work/guan-yu-ben-wang-zhan.html">关于本网站</a>
                        </li>
                        <li >
                            <a href="https://docs.cdwanze.work/gong-gao.html">公告</a>
                        </li>
            </ul>


        </div>
    </div>
</nav>


<div class="container-fluid">
    <div class="col-md-1 col-md-1-left"></div>
    <div class="col-md-10">
<article>
<div class="row">
    <header class="page-header col-md-10 col-md-offset-2">
    <h1><a href="https://docs.cdwanze.work/articles/scrapy-module.html"> scrapy模块  </a></h1>
    </header>
</div>

<div class="row">
    <div class="col-md-2 table-of-content">
        <nav>
        <h4>目录</h4>
        <div class="toc">
<ul>
<li><a href="#_1">简介</a><ul>
<li><a href="#_2">新建一个项目</a></li>
</ul>
</li>
<li><a href="#_3">第一个例子</a><ul>
<li><a href="#response">response对象</a></li>
</ul>
</li>
<li><a href="#_4">测试抓取</a></li>
<li><a href="#_5">实际爬取</a></li>
<li><a href="#xpath">xpath语法</a><ul>
<li><a href="#title">选择title</a></li>
<li><a href="#title_1">选择title包含的文本</a></li>
<li><a href="#id">按照id选择</a></li>
<li><a href="#_6">继续往下选</a></li>
<li><a href="#_7">选择目标标签的属性</a></li>
<li><a href="#_8">选择属性</a></li>
<li><a href="#_9">选择文本</a></li>
<li><a href="#string">string</a></li>
</ul>
</li>
<li><a href="#_10">减缓访问速度</a></li>
<li><a href="#_11">控制输出</a></li>
<li><a href="#scrapy">慎用scrapy的高级特性</a></li>
<li><a href="#_12">模拟用户登录</a><ul>
<li><a href="#cookies">登录的cookies问题</a></li>
</ul>
</li>
<li><a href="#_13">防止被封的策略</a><ul>
<li><a href="#http">http请求头调整</a></li>
<li><a href="#cookies_1">Cookies</a></li>
<li><a href="#_14">表单陷阱</a></li>
<li><a href="#403-forbidden">403 forbidden</a></li>
</ul>
</li>
<li><a href="#_15">高级议题</a><ul>
<li><a href="#jsonpipeline">JsonPipeline</a></li>
<li><a href="#imagespipeline">ImagesPipeline</a></li>
<li><a href="#mongodbpipeline">MongoDBPipeline</a></li>
<li><a href="#settings">settings的传递</a></li>
</ul>
</li>
</ul>
</div>
        </nav>
    </div>
    <div class="col-md-8 article-content">

            
            
<h2 id="_1">简介</h2>
<p>Scrapy模块和Django模块在使用上包括在内容目录结构上都很相似，当然这两个项目干的完全是两个不同的事情，我想这种相似性更多的可以描述为类似于大多数GUI界面的那种类似。</p>
<p>我之前尝试过写过一个小的Spider网络爬虫程序，其实网络爬取大体过程都是类似的，因此我学习Scrapy项目大体只是一些基本配置的了解，对于其内部原理已经很熟悉了，所以本文不会在这些地方赘述了。</p>
<p>scrapy就是一个python模块，可以通过 <code>pip</code> 安装之，所以安装这块也不多说了。</p>
<h3 id="_2">新建一个项目</h3>
<div class="highlight"><pre><span></span><span class="n">scrapy</span><span class="w"> </span><span class="n">startproject</span><span class="w"> </span><span class="n">project_name</span><span class="w">  </span><span class="o">[</span><span class="n">path</span><span class="o">]</span><span class="w"> </span>
</pre></div>
<p>因为我喜欢创建python的venv虚拟环境，所以上面最后path设置为 <code>.</code> 当前目录下的意思。</p>
<p>然后接下来就是编写爬虫Spider脚本还有等等其他一些配置了。</p>
<h2 id="_3">第一个例子</h2>
<p>下面是一个简单的例子：</p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">user_agent</span> <span class="o">=</span> <span class="n">get_random_user_agent</span><span class="p">()</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">"7yrt"</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'http://7yrt.com/html/rhrt/'</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">url</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span>
        <span class="n">html</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">text</span>

        <span class="c1">## do something</span>
        <span class="k">if</span> <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="s1">'http://7yrt.com/html/rhrt/[\d]+/[\d]+/[\d_]+.html'</span><span class="p">,</span> <span class="n">url</span><span class="p">):</span>
            <span class="n">images</span> <span class="o">=</span> <span class="n">parse_webpage_images</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">html</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">'div'</span><span class="p">,</span> <span class="n">class_</span><span class="o">=</span><span class="s1">'imgview'</span><span class="p">)</span>
            <span class="n">title</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">'//h1/text()'</span><span class="p">)</span><span class="o">.</span><span class="n">extract_first</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">index</span><span class="p">,</span><span class="n">image</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">images</span><span class="p">):</span>
                <span class="k">yield</span> <span class="n">MyItem</span><span class="p">(</span><span class="n">uuid</span><span class="o">=</span><span class="n">get_item_uuid</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">index</span><span class="p">)),</span>
                            <span class="n">image_url</span><span class="o">=</span><span class="p">[</span><span class="n">image</span><span class="p">],</span>
                            <span class="n">name</span> <span class="o">=</span> <span class="n">title</span><span class="p">)</span>


        <span class="c1">##### get next page</span>
        <span class="n">links</span> <span class="o">=</span> <span class="n">parse_webpage_links</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">html</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">next_page</span> <span class="ow">in</span> <span class="n">links</span><span class="p">:</span>
            <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">next_page</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
</pre></div>
<ul>
<li>
<p><code>user_agent</code> 属性改变你爬虫情况的USER_AGENT HTTP头，这通常需要设置一下，防止你的爬虫被ban。</p>
</li>
<li>
<p><code>name</code> 你的爬虫的名字，等下你要具体运行某个爬虫的名字是 <code>scrapy crawl spider_name</code> ，用的就是这里定义的名字。然后 <code>scrapy list</code> 显示的也是这些爬虫名字。</p>
</li>
<li>
<p><code>start_urls</code> 你的爬虫起始开爬点，官方教程提到过 <code>start_requests</code> 这个方法，一般就定义 <code>start_urls</code> 还是很简便的。</p>
</li>
</ul>
<h3 id="response">response对象</h3>
<p>response对象可以获取 <code>response.text</code> 然后送给beautifulsoup来处理，比如上面的 <code>parse_webpage_images</code> 和 <code>parse_webpage_links</code> 就是这样做的，主要是这两个以前写的函数还是很简便的，所以就没考虑效率问题了，有的时候真的不在乎那么一点，因为后面还会讨论减缓爬虫爬取速度的问题。</p>
<p>然后官方教程提到的response对象可以调用 <code>css</code> 或 <code>xpath</code> 方法来进行一些信息提取工作，这个简单了解下xpath语法，还是很便捷的。</p>
<h2 id="_4">测试抓取</h2>
<div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">shell</span>  <span class="n">url</span>
</pre></div>
<p>然后进入shell之后，有个 <code>response</code> 对象，其对应于之前写爬虫 parse函数时候的那个response对象。进一步可以做一些前期测试抓取的工作。</p>
<h2 id="_5">实际爬取</h2>
<p>一般实际爬取某个爬虫使用的是 <code>scrapy crawl</code> 命令：</p>
<p>你可以如下通过 <code>-a</code> 给你的爬虫传递一些参数进去</p>
<div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">crawl</span> <span class="n">spider_51shucheng</span>  <span class="o">-</span><span class="n">a</span> <span class="n">book_name</span><span class="o">=</span><span class="ss">"xunqinji"</span>
</pre></div>
<p>参数到了爬虫那边是传递给了 <code>__init__</code> 哪里：</p>
<div class="highlight"><pre><span></span><span class="n">def</span> <span class="n">__init__</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">book_name</span><span class="o">=</span><span class="s1">''</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</pre></div>
<p>推荐采用笔者的这种处理方式，实践表明这很优雅灵活，就是通过 <code>-a</code> 传递极少的参数，从而确定你的爬虫要寻找的 <code>.ini</code> 配置文件所在，然后加载这个配置文件进一步完成从 <code>start_url</code> 到后面的爬虫动作的各个参数调配。</p>
<h2 id="xpath">xpath语法</h2>
<p>下面主要通过各个例子简要介绍xpath语法之，参考了 <a href="http://www.ruanyifeng.com/blog/2009/07/xpath_path_expressions.html">阮一峰的这篇文章</a> 和菜鸟教程的xpath教程。</p>
<p>基本东西简单了解下即可，然后多看例子吧。</p>
<div class="highlight"><pre><span></span><span class="o">/</span><span class="n">what</span>   <span class="err">基本路径表达，下个节点</span>
<span class="o">//</span><span class="n">what</span>  <span class="err">基本路径表达，任意位置的下个节点</span>
</pre></div>
<p>这里 <code>/</code> 表示在下个节点中匹配， <code>//</code> 下个或所有子节点匹配。 </p>
<div class="highlight"><pre><span></span><span class="o">//</span><span class="n">div</span><span class="o">[</span><span class="n">@id='what'</span><span class="o">]</span><span class="w">   </span><span class="n">根据id定位</span><span class="w"></span>
<span class="o">//</span><span class="n">div</span><span class="o">[</span><span class="n">@id='what'</span><span class="o">]/</span><span class="n">a</span><span class="o">[</span><span class="n">1</span><span class="o">]</span><span class="w"> </span><span class="n">根据id定位后找下面的第一个a标签</span><span class="w"></span>
<span class="o">//</span><span class="n">div</span><span class="o">[</span><span class="n">@id='what'</span><span class="o">]/</span><span class="n">a</span><span class="o">[</span><span class="n">*</span><span class="o">]</span><span class="w"> </span><span class="n">根据id定位后找下面的所有a标签</span><span class="w"></span>
</pre></div>
<p>这里 <code>*</code> 表示所有的意思。</p>
<div class="highlight"><pre><span></span><span class="o">//</span><span class="n">div</span><span class="o">[</span><span class="n">@name</span><span class="o">]</span><span class="w">   </span><span class="n">找具有name属性的div标签</span><span class="w"></span>

<span class="o">//</span><span class="n">div</span><span class="o">[</span><span class="n">@name='what'</span><span class="o">]</span><span class="w"> </span><span class="n">找name属性等于what的div标签</span><span class="w"> </span>

<span class="o">/</span><span class="cm">/*[contains(@class,'what')] 找某个标签class属性有 what NOTICE: 这里的意思是有，多个class属性也是可以匹配的 class="what what_what"</span>
<span class="cm">//div[@class='what'] 那个目标标签的class属性就是what，也就是匹配的是 class="what"</span>

<span class="cm">//*[@id="list"]//dd[*]/a[@href and @title] 找id=list的标签下面的所有dd标签下面的a标签，a标签必须有href和title属性。</span>
</pre></div>
<div class="highlight"><pre><span></span><span class="o">//</span><span class="n">title</span><span class="p">[</span><span class="o">@*</span><span class="p">]</span>  <span class="err">选择</span><span class="n">title</span><span class="err">，随意属性，但</span><span class="n">title标签必须有属性</span>
</pre></div>
<h3 id="title">选择title</h3>
<div class="highlight"><pre><span></span><span class="o">//</span><span class="n">title</span>
</pre></div>
<p>这是选择到了文档中任意位置的 title 标签， <code>/</code> 开头的话会选择根节点，这不太好用。</p>
<h3 id="title_1">选择title包含的文本</h3>
<div class="highlight"><pre><span></span><span class="o">//</span><span class="n">title</span><span class="o">/</span><span class="nb">text</span><span class="p">()</span>
</pre></div>
<h3 id="">按照id选择</h3>
<div class="highlight"><pre><span></span><span class="o">//</span><span class="n">div</span><span class="o">[</span><span class="n">@id='post-date'</span><span class="o">]/</span><span class="nc">text</span><span class="p">()</span><span class="w"></span>
</pre></div>
<p>上面例子的意思是选择一个div标签，其有id属性 <code>post-date</code> ，如果div改为 <code>*</code> 则为随便什么标签名字。</p>
<h3 id="_6">继续往下选</h3>
<div class="highlight"><pre><span></span><span class="o">/</span><span class="cm">/*[@id='js_profile_qrcode']/div/p[1]/span/text()</span>
</pre></div>
<h3 id="_7">选择目标标签的属性</h3>
<div class="highlight"><pre><span></span><span class="o">///</span><span class="cm">/*[@id='js_profile_qrcode']//a/@href</span>
</pre></div>
<h3 id="_8">选择属性</h3>
<div class="highlight"><pre><span></span><span class="o">/</span><span class="cm">/*[@id="list"]//dd[*]/a[@href and @title]/@href  </span>
</pre></div>
<h3 id="_9">选择文本</h3>
<div class="highlight"><pre><span></span><span class="o">//</span><span class="n">title</span><span class="o">/</span><span class="nb">text</span><span class="p">()</span>
</pre></div>
<h3 id="string">string</h3>
<p>对于选择的节点（注意如果返回的是节点集 nodeset将只取第一个），将所有的节点（也就是包括子节点）的文本抽取出来并合并。</p>
<div class="highlight"><pre><span></span><span class="n">string</span><span class="p">(</span><span class="o">//</span><span class="n">div</span><span class="o">[</span><span class="n">@class="lemma-summary"</span><span class="o">]</span><span class="p">)</span><span class="w"></span>
</pre></div>
<h2 id="_10">减缓访问速度</h2>
<p>在网络爬取中，防止被ban（一般403错误就是由此而来）一直是个大问题。开代理换IP成本挺高的，不过下面这些手段一般还是能够实现的，这些都在settings.py里面就有了，只需要去注释就是了。大体有下面这些：</p>
<div class="highlight"><pre><span></span><span class="n">DOWNLOAD_DELAY</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">CONCURRENT_REQUESTS_PER_DOMAIN</span> <span class="o">=</span> <span class="mi">16</span>
<span class="o">#</span><span class="n">CONCURRENT_REQUESTS_PER_IP</span> <span class="o">=</span> <span class="mi">16</span>
<span class="o">#</span> <span class="n">Disable</span> <span class="n">cookies</span> <span class="p">(</span><span class="n">enabled</span> <span class="k">by</span> <span class="k">default</span><span class="p">)</span>
<span class="n">COOKIES_ENABLED</span> <span class="o">=</span> <span class="k">False</span>
</pre></div>
<p>就是设置下载访问停顿时间和并行请求数还有禁用cookies。除了禁用cookies之外，上面这几个设置可以不用，请看到官方文档的 <a href="https://doc.scrapy.org/en/latest/topics/autothrottle.html">这里</a> 。</p>
<p>这个在 settings.py 文件的后面些也有，这是一种自动节流机制，它是利用下载延迟还有并行数来自动调节DELAY时间，</p>
<div class="highlight"><pre><span></span><span class="n">AUTOTHROTTLE_ENABLED</span> <span class="o">=</span> <span class="k">True</span>
<span class="o">#</span> <span class="n">The</span> <span class="n">initial</span> <span class="n">download</span> <span class="n">delay</span>
<span class="n">AUTOTHROTTLE_START_DELAY</span> <span class="o">=</span> <span class="mi">5</span>
<span class="o">#</span> <span class="n">The</span> <span class="n">maximum</span> <span class="n">download</span> <span class="n">delay</span> <span class="k">to</span> <span class="n">be</span> <span class="k">set</span> <span class="k">in</span> <span class="k">case</span> <span class="k">of</span> <span class="n">high</span> <span class="n">latencies</span>
<span class="n">AUTOTHROTTLE_MAX_DELAY</span> <span class="o">=</span> <span class="mi">60</span>
<span class="o">#</span> <span class="n">The</span> <span class="n">average</span> <span class="nb">number</span> <span class="k">of</span> <span class="n">requests</span> <span class="n">Scrapy</span> <span class="n">should</span> <span class="n">be</span> <span class="n">sending</span> <span class="k">in</span> <span class="n">parallel</span> <span class="k">to</span>
<span class="o">#</span> <span class="k">each</span> <span class="n">remote</span> <span class="n">server</span>
<span class="n">AUTOTHROTTLE_TARGET_CONCURRENCY</span> <span class="o">=</span> <span class="mi">1</span><span class="p">.</span><span class="mi">0</span>
</pre></div>
<p>最后要说的是自动节流和前面的 <code>DOWNLOAD_DELAY</code> 和 <code>CONCURRENT_REQUESTS_PER_DOMAIN</code> 是协作关系。自动节流最小不会小过 <code>DOWNLOAD_DELAY</code> ，最大不会大过 <code>AUTOTHROTTLE_MAX_DELAY</code> 。 然后 <code>AUTOTHROTTLE_TARGET_CONCURRENCY</code> 也只是一个节流建议，并不是最大极限，对于单个域名的最大并行请求数是由 <code>CONCURRENT_REQUESTS_PER_DOMAIN</code> 定义的。</p>
<h2 id="_11">控制输出</h2>
<p><code>scrapy crawl</code> 命令默认 <code>-o test.json</code> 参数你可以省略，然后settings.py 哪里控制你的输出格式和文件名等等，具体请参看scrapy官方文档的 feed-export 一章。</p>
<p>比如你可以通过如下配置来做到默认输出jsonlines格式，这个对于爬虫数据收集来说会更好一点，容错率高一点：</p>
<div class="highlight"><pre><span></span><span class="n">FEED_URI</span> <span class="o">=</span> <span class="s1">'file:///output/%(name)s/%(time)s.jl'</span>
<span class="n">FEED_FORMAT</span> <span class="o">=</span> <span class="s1">'jsonlines'</span>
<span class="n">FEED_EXPORT_ENCODING</span> <span class="o">=</span> <span class="s1">'utf8'</span>
</pre></div>
<h2 id="scrapy">慎用scrapy的高级特性</h2>
<p>请慎用scrapy的高级特性，什么PIpeline啊，middleware等，请慎用数据库。如果不是简单的练手性质项目，而是正式的爬虫项目，够你操心的事将是一大堆，不管怎么样，先把数据爬到手再说。弄那些高级特性和数据库只会让你的精力被那些吸走，你的关注点应该集中在爬取，xpath分析，必要的数据收集和数据的后处理上。</p>
<h2 id="_12">模拟用户登录</h2>
<p>表单简单来说就是一个前端友好的界面，其实质就是发送了 一个 POST 请求。</p>
<p>关键是要理解前端的表单界面，具体POST了哪些参量。</p>
<div class="highlight"><pre><span></span><span class="nt">&lt;form</span> <span class="na">method=</span><span class="s">"POST"</span> <span class="na">action=</span><span class="s">"???"</span>
    <span class="err">&lt;input</span> <span class="na">type=</span><span class="s">...</span> <span class="na">name=</span><span class="s">"firstname"</span> <span class="err">...</span> <span class="nt">&gt;</span>
    <span class="nt">&lt;input</span> <span class="err">type</span> <span class="err">...</span>
    <span class="err">submit</span>
<span class="err">&lt;/form</span><span class="nt">&gt;</span>
</pre></div>
<p>input 的  <strong>name</strong> 就是具体 POST 的参量，然后就是action那边就是你要 POST 的目的地。</p>
<p>类似的表单还有很多其他元素，比如checkbox之类的，其都不过是为了让用户快速地设置某个参量罢了。</p>
<h3 id="cookies">登录的cookies问题</h3>
<p>因为http无状态，所以有cookies 和 session ，服务端数据库记录session，cookies就是客户端。爬虫要正常登录，记得保留好登录成功之后的cookies。</p>
<p>多次请求的时候，用requests的cookies保存和设置就不好用了。记得使用requests的session机制。</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">requests</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'username'</span><span class="p">:</span><span class="s1">'user'</span><span class="p">,</span> <span class="s1">'password'</span><span class="p">:</span> <span class="s1">'123456'</span> <span class="p">}</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="s1">'login.html'</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>

<span class="c1">##### 继续用这个session来请求，没有任何问题</span>
<span class="n">s</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">'profile.html'</span><span class="p">)</span>
</pre></div>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">LoginSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">'example.com'</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'http://www.example.com/users/login.php'</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">FormRequest</span><span class="o">.</span><span class="n">from_response</span><span class="p">(</span>
            <span class="n">response</span><span class="p">,</span>
            <span class="n">formdata</span><span class="o">=</span><span class="p">{</span><span class="s1">'username'</span><span class="p">:</span> <span class="s1">'john'</span><span class="p">,</span> <span class="s1">'password'</span><span class="p">:</span> <span class="s1">'secret'</span><span class="p">},</span>
            <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">after_login</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">after_login</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="c1"># check login succeed before going on</span>
        <span class="k">if</span> <span class="s2">"authentication failed"</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">body</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">"Login failed"</span><span class="p">)</span>
            <span class="k">return</span>

        <span class="c1"># continue scraping with authenticated session...</span>
</pre></div>
<h2 id="_13">防止被封的策略</h2>
<ol>
<li>设置随机的user agent策略</li>
<li>禁用cookie <code>COOKIES_ENABLED = True</code></li>
<li>设置下载停顿 <code>DOWNLOAD_DELAY = n</code></li>
<li>使用代理池</li>
</ol>
<p>防止被封的首要策略就是尽量的为别人的服务器多考虑下，让自己写的爬虫少请求，每次请求都是有效的核心请求获取最核心的数据，不管是刷页面还是刷ajax，多次请求之间应该设置一个停顿时间。</p>
<div class="highlight"><pre><span></span><span class="n">time</span><span class="p">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
<p>在上面的首要原则的基础上，下面介绍的很多实战技巧，其实都符合一个大的原则：尽可能地让你的爬虫和人浏览网页没有区别。</p>
<h3 id="http">http请求头调整</h3>
<p>user-agent 设置，而且时不时的切换下。</p>
<p>虽然目前还没有遇到，不过我推测可能 Referrer 这个header在某些场景下是有些文章的。</p>
<p>还有 Accept-Language 也可能有用。</p>
<h3 id="cookies_1">Cookies</h3>
<p>有些情况下cookies的正常获取需要javascript的支持。cookies总的原则是第一次请求获取到cookies，然后后面的很多次请求都使用这个cookies即可。</p>
<p>不过反爬虫cookies一般都会有个时间限制，一个简单的做法就是这边也设置个时间，定时获取最新的cookies或者，一定请求量之后再获取一个新的cookies。</p>
<p>具体使用 scrapy-splash 了解下。</p>
<h3 id="_14">表单陷阱</h3>
<p>有的表单里面有：</p>
<div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="k">input</span> <span class="k">type</span><span class="o">=</span><span class="ss">"hidden"</span> <span class="p">...</span>
</pre></div>
<p>我们要记住人如果在页面上点击，这个没有显示的字段的值也会一并送过去，而他们服务器那边会根据这个值可能是个加密的某个值来判断这个请求是人点的还是爬虫行为。</p>
<p>最好的策略是先把整个表单内容爬过来，收集好之后再发送表单请求。</p>
<p>和上面的情况相反，还有一种情况，页面表单发送可能有特别的处理，某些表单字段，不管用户看得见看不见，你都不能发送过去，只要发送过去就会被毙掉。</p>
<p>继续上面的表单陷阱，某些css也会动态将某个input 属于hidden属性，这个需要好好分析下。</p>
<h3 id="403-forbidden">403 forbidden</h3>
<p>这极有可能是你的爬虫被封了。</p>
<h2 id="_15">高级议题</h2>
<h3 id="jsonpipeline">JsonPipeline</h3>
<p><code>pipelines.py</code> 文件里面就定义了一些你自己写的Pipeline类，比如下面这个是JsonPipeline类：</p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">JsonPipeline</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">file</span> <span class="o">=</span> <span class="n">codecs</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">'test.json'</span><span class="p">,</span> <span class="s1">'w'</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">'utf-8'</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="n">line</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">item</span><span class="p">),</span> <span class="n">ensure_ascii</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span> <span class="o">+</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">item</span>

    <span class="k">def</span> <span class="nf">spider_closed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">file</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
<p>大体就是一个简单的类，其中一些特别的方法有特别的用处。这个jsonpipelie并不具有实用价值，简单了解下即可。</p>
<h3 id="imagespipeline">ImagesPipeline</h3>
<p>想要自动下载图片，没问题，scrapy已经内置有这个功能了！你需要做的就是收集好图片连接就是了。设置里要加上这样一行：</p>
<div class="highlight"><pre><span></span><span class="n">ITEM_PIPELINES</span> <span class="o">=</span> <span class="err">{</span>
    <span class="s1">'scrapy.pipelines.images.ImagesPipeline'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
<span class="err">}</span>
</pre></div>
<p>然后设置里还有如下相关配置:</p>
<div class="highlight"><pre><span></span><span class="n">IMAGES_STORE</span> <span class="o">=</span> <span class="s1">'/path/to/download_images'</span>
<span class="n">IMAGES_URLS_FIELD</span> <span class="o">=</span> <span class="s1">'image_url'</span>
<span class="n">IMAGES_RESULT_FIELD</span> <span class="o">=</span> <span class="s1">'image'</span>
</pre></div>
<p>这里 <code>IMAGES_URLS_FIELD</code> 的默认值是 <code>image_urls</code> ，你需要在你的items对象加上这一属性，其是一个列表值。然后 <code>IMAGES_RESULT_FIELD</code> 默认值是 <code>images</code> ， 这个值ImagesPipeline会自动填充，不需要管的。这里改名字是因为我不喜欢很多图片混在一起，所以做了一些处理分开了。</p>
<h3 id="mongodbpipeline">MongoDBPipeline</h3>
<p>想要把数据直接实时填入到mongodb里面去？用 <code>MongoDBPipeline</code> 即可，你需要</p>
<div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">scrapy</span><span class="o">-</span><span class="n">mongodb</span>
</pre></div>
<p>然后配置加上</p>
<div class="highlight"><pre><span></span><span class="n">ITEM_PIPELINES</span> <span class="o">=</span> <span class="err">{</span>
    <span class="s1">'scrapy_mongodb.MongoDBPipeline'</span><span class="p">:</span> <span class="mi">400</span><span class="p">,</span>
<span class="err">}</span>
</pre></div>
<p>这后面的数字只是执行优先级，没什么特别的含义。</p>
<p>然后还有配置：</p>
<div class="highlight"><pre><span></span><span class="n">MONGODB_URI</span> <span class="o">=</span> <span class="s1">'mongodb://localhost:27017'</span>
<span class="n">MONGODB_DATABASE</span> <span class="o">=</span> <span class="s1">'myscrapy'</span>
<span class="n">MONGODB_UNIQUE_KEY</span> <span class="o">=</span> <span class="s1">'uuid'</span>
</pre></div>
<p>这个插件的 <code>MONGODB_COLLECTION</code> 值默认是 <code>items</code> 是个死的，我还不是很满意。然后 <code>MONGODB_UNIQUE_KEY</code> 我还不清楚是什么，后面有时间继续。</p>
<h3 id="settings">settings的传递</h3>
<p>爬虫初始化后， <code>self.settings</code> 就可以使用了，通过它你就可以调用一些在 <code>settings.py</code> 文件里面的配置变量了。然后你在写pipeline的时候，如下：</p>
<div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">open_spider</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">client</span> <span class="o">=</span> <span class="n">MongoClient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mongo_uri</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mongodb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mongo_dbname</span><span class="p">]</span>
        <span class="n">spider</span><span class="o">.</span><span class="n">mongodb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mongodb</span>
</pre></div>
<p><code>open_spider</code> 是打开爬虫后的动作，定义 <code>self.mongodb</code> 是将目标mongodb 数据库对象挂载本 pipeline上，而 <code>spider.mongodb</code> 是将这个变量挂在本爬虫上，这样后面你的爬虫类那里都是可以直接用 <code>self.mongodb</code> 来调用目标变量的，但说到爬虫类 <code>__init__</code> 方法里面还不大确定。然后你写pipeline的时候通过 <code>crawler.settings</code> 也可以或者配置变量：</p>
<div class="highlight"><pre><span></span>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_crawler</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">crawler</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span>
            <span class="n">mongo_uri</span><span class="o">=</span><span class="n">crawler</span><span class="o">.</span><span class="n">settings</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">'MONGODB_URI'</span><span class="p">),</span>
            <span class="n">mongo_dbname</span><span class="o">=</span><span class="n">crawler</span><span class="o">.</span><span class="n">settings</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">'MONGODB_DATABASE'</span><span class="p">)</span>
        <span class="p">)</span>
</pre></div>
            
            
            <hr/>

        </div>
        <section>
        <div class="col-md-2" style="float:right;font-size:0.9em;">
            <h4>首发于：</h4>
            <time pubdate="pubdate" datetime="2017-10-11T00:00:00+08:00">2017年 10月 11日 </time>

<h4>最近更新于：</h4>
<time datetime="2019-10-08T00:00:00+08:00">2019年 10月 8日 </time>

            <h4>分类：</h4>
            <a class="category-link" href="https://docs.cdwanze.work/categories.html#pythonhao-huo-ban-ref">python好伙伴</a>
            <h4>标签：</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="https://docs.cdwanze.work/tags.html#python-ref">python
                    <span>14</span>
</a></li>
                <li><a href="https://docs.cdwanze.work/tags.html#wang-luo-zhua-qu-ref">网络抓取
                    <span>2</span>
</a></li>
            </ul>
        </div>
        </section>
</div>
</article>
    </div>
    <div class="col-md-1"></div>

</div>


<div id="push"></div>


<footer>
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-power">Powered by <a href="http://getpelican.com/" title="Pelican Home Page">Pelican</a>. Theme: <a href="http://oncrashreboot.com/pelican-elegant" title="Theme Elegant Home Page">Elegant</a> by <a href="http://oncrashreboot.com" title="Talha Mansoor Home Page">Talha Mansoor</a> and updated by <a href="http://www.cdwanze.work" title="cdwanze Home Page">cdwanze</a></li>
    </ul>
</div>
</footer>

        <script src="https://docs.cdwanze.work/theme/js/jquery.min.js"></script>
    <script src="https://docs.cdwanze.work/theme/js/bootstrap.min.js"></script>
    <script>
        function validateForm(query) {
            return (query.length > 0);
        }
    </script>

    <script>
        function adjust_search_width() {
            var w = document.documentElement.clientWidth;
            if ((w > 755) && (w < 975)) {
                plus_width = w - 755;
                $('.navbar-form .form-control').outerWidth(210 + plus_width);
            } else if (w >= 975) {
                $('.navbar-form .form-control').outerWidth(210 + 220);
            } else if (w <= 755) {
                $('.navbar-form .form-control').css('width', '100%')
            }
        }

        $(document).ready(function () {
            adjust_search_width();
        });

        window.onresize = function () {
            adjust_search_width();
        }

    </script>


    



</body>
</html>