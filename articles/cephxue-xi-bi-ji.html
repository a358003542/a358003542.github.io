<!DOCTYPE html>
<html lang="zh-cn">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="msvalidate.01" content="55CB117A61A6F8286173763FB18D9625"/>

        <meta name="author" content="wanze"/>
        <meta name="copyright" content="wanze"/>

        <meta name="description"
              content="ceph简介 ceph系统是一种分布式存储解决方案，可以通过多台计算机进行配置，搭建ceph集群并提供块设备存储、对象存储和文件系统服务。 ceph项目的设计思想是很先进的，其源于Sage Weil在加州大学攻读博士期间的一篇论文。ceph项目简单来说目的就是应对PB级别的存储需求的，当然ceph在设计上是没有存储理论上限的。 正如ceph官网所言： Ceph is a unified, distributed storage system designed for excellent performance, reliability and scalability. ceph是一个分布式存储系统，提供了优良的性能，可靠性和可扩展性。可靠性就是ceph存储系统会尽可能地保证数据不会丢失，可扩展性包括系统规模和存储容量的可扩展。此外ceph在运维上还提供了自动化运维特性，包括数据自动replication；自动re-balancing；自动failure detection和自动failure recoery。 ceph存储系统结构 ceph存储系统如下图所示分为四个层次： 首先是最底层的RADOS基础存储系统，所有存储在ceph系统的用户数据最终都是由这一层来存储的。理解RADOS是理解ceph系统的关键。 基础库librados，这一层对底层RADOS系统进行抽象和封装，并向上提供API。 高层应用接口，这一层有RADOS GW、RBD、ceph FS …
"/>

        <meta property="og:type" content="article"/>
        <meta name="twitter:card" content="summary">

<meta name="keywords" content=", others, " />

<meta property="og:title" content="ceph学习笔记 "/>
<meta property="og:url" content="https://a358003542.github.io/articles/cephxue-xi-bi-ji.html" />
<meta property="og:description" content="ceph简介 ceph系统是一种分布式存储解决方案，可以通过多台计算机进行配置，搭建ceph集群并提供块设备存储、对象存储和文件系统服务。 ceph项目的设计思想是很先进的，其源于Sage Weil在加州大学攻读博士期间的一篇论文。ceph项目简单来说目的就是应对PB级别的存储需求的，当然ceph在设计上是没有存储理论上限的。 正如ceph官网所言： Ceph is a unified, distributed storage system designed for excellent performance, reliability and scalability. ceph是一个分布式存储系统，提供了优良的性能，可靠性和可扩展性。可靠性就是ceph存储系统会尽可能地保证数据不会丢失，可扩展性包括系统规模和存储容量的可扩展。此外ceph在运维上还提供了自动化运维特性，包括数据自动replication；自动re-balancing；自动failure detection和自动failure recoery。 ceph存储系统结构 ceph存储系统如下图所示分为四个层次： 首先是最底层的RADOS基础存储系统，所有存储在ceph系统的用户数据最终都是由这一层来存储的。理解RADOS是理解ceph系统的关键。 基础库librados，这一层对底层RADOS系统进行抽象和封装，并向上提供API。 高层应用接口，这一层有RADOS GW、RBD、ceph FS …" />
<meta property="og:site_name" content="wanze的博文" />
<meta property="og:article:author" content="wanze" />
<meta property="og:article:published_time" content="2020-05-24T16:53:27.929151+08:00" />
<meta name="twitter:title" content="ceph学习笔记 ">
<meta name="twitter:description" content="ceph简介 ceph系统是一种分布式存储解决方案，可以通过多台计算机进行配置，搭建ceph集群并提供块设备存储、对象存储和文件系统服务。 ceph项目的设计思想是很先进的，其源于Sage Weil在加州大学攻读博士期间的一篇论文。ceph项目简单来说目的就是应对PB级别的存储需求的，当然ceph在设计上是没有存储理论上限的。 正如ceph官网所言： Ceph is a unified, distributed storage system designed for excellent performance, reliability and scalability. ceph是一个分布式存储系统，提供了优良的性能，可靠性和可扩展性。可靠性就是ceph存储系统会尽可能地保证数据不会丢失，可扩展性包括系统规模和存储容量的可扩展。此外ceph在运维上还提供了自动化运维特性，包括数据自动replication；自动re-balancing；自动failure detection和自动failure recoery。 ceph存储系统结构 ceph存储系统如下图所示分为四个层次： 首先是最底层的RADOS基础存储系统，所有存储在ceph系统的用户数据最终都是由这一层来存储的。理解RADOS是理解ceph系统的关键。 基础库librados，这一层对底层RADOS系统进行抽象和封装，并向上提供API。 高层应用接口，这一层有RADOS GW、RBD、ceph FS …">


    <title>ceph学习笔记  · wanze的博文
</title>

        <link href="https://a358003542.github.io/theme/css/font-awesome.css" rel="stylesheet"
              media="screen">
        <link href="https://a358003542.github.io/theme/css/bootstrap.min.css" rel="stylesheet"
              media="screen">

        <link rel="stylesheet" type="text/css"
                  href="https://a358003542.github.io/theme/css/pygments.css" media="screen">
        <link rel="stylesheet" type="text/css"
                  href="https://a358003542.github.io/theme/css/elegant.css" media="screen">
        <link rel="stylesheet" type="text/css"
                  href="https://a358003542.github.io/theme/css/base.css" media="screen">




</head>
<body>

<nav class="navbar">
    <div class="navbar navbar-default" role="navigation">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed"
                    data-toggle="collapse" data-target=".navbar-collapse"
                    aria-expanded="false">
                <span class="sr-only">Navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>

            <a class="navbar-brand" href="https://a358003542.github.io/"><span
                    class=site-name>网站首页</span></a>
        </div>


        <div class="navbar-collapse collapse">
            <form action="https://a358003542.github.io/search.html"
                  onsubmit="return validateForm(this.elements['q'].value);"
                  class="navbar-form navbar-left">
                <div class="form-group">
                    <input type="text" name="q" id="tipue_search_input"
                           class="form-control" placeholder="Search..."
                           style="width:430px;">
                </div>
                <button class="btn btn-default" type="submit">搜索</button>
            </form>


            <ul class="nav navbar-nav nav-pills navbar-right">
                <li >
                    <a  href="/archives.html">所有文章</a></li>

                <li ><a href="/categories.html">文章分类</a></li>
                <li ><a href="/tags.html">文章标签</a></li>


                        <li >
                            <a href="https://a358003542.github.io/about.html">关于本网站</a>
                        </li>
            </ul>


        </div>
    </div>
</nav>


<div class="container-fluid">
    <div class="col-md-1 col-md-1-left"></div>
    <div class="col-md-10">
<article>
<div class="row">
    <header class="page-header col-md-10 col-md-offset-2">
    <h1><a href="https://a358003542.github.io/articles/cephxue-xi-bi-ji.html"> ceph学习笔记  </a></h1>
    </header>
</div>

<div class="row">
    <div class="col-md-2 table-of-content">
        <nav>
        <h4>目录</h4>
        <div class="toc">
<ul>
<li><a href="#ceph">ceph简介</a><ul>
<li><a href="#ceph_1">ceph存储系统结构</a></li>
<li><a href="#rados">RADOS存储系统结构</a></li>
<li><a href="#ceph_2">ceph存储集群</a></li>
</ul>
</li>
<li><a href="#_1">安装前的预检</a><ul>
<li><a href="#adminceph-deploy">在admin节点安装ceph-deploy</a></li>
<li><a href="#ceph_3">关闭各个ceph节点防火墙</a></li>
<li><a href="#ceph_4">配置各个ceph节点主机名</a></li>
<li><a href="#ntp">配置NTP</a><ul>
<li><a href="#ntp_1">安装NTP服务</a></li>
</ul>
</li>
<li><a href="#ntp_2">NTP服务启动</a></li>
<li><a href="#ceph_5">创建部署ceph的用户</a></li>
<li><a href="#ssh">配置ssh免密登录</a></li>
<li><a href="#selinux">关闭SELinux</a></li>
<li><a href="#epel">配置epel源</a></li>
<li><a href="#ceph_6">配置ceph镜像源</a></li>
<li><a href="#_2">网络环境预检</a></li>
</ul>
</li>
<li><a href="#cephconf">ceph.conf的配置</a><ul>
<li><a href="#_3">具体配置的覆盖</a></li>
<li><a href="#global">global</a></li>
<li><a href="#_4">网络配置最小要求</a></li>
</ul>
</li>
<li><a href="#_5">硬盘和文件系统配置</a></li>
<li><a href="#mon">mon的配置</a><ul>
<li><a href="#mon_1">mon的部署基本流程</a></li>
<li><a href="#mon_2">mon之间的同步</a></li>
</ul>
</li>
<li><a href="#osd">osd的配置</a><ul>
<li><a href="#osd_1">osd的心跳</a></li>
<li><a href="#osd_2">osd的日志</a></li>
</ul>
</li>
<li><a href="#_6">认证配置</a><ul>
<li><a href="#cephx">开启cephx</a></li>
<li><a href="#_7">术语</a></li>
<li><a href="#metavariable">METAVARIABLE</a></li>
<li><a href="#_8">运行时更改配置</a></li>
<li><a href="#_9">运行时查看配置</a></li>
</ul>
</li>
<li><a href="#diamond">安装diamond模块</a></li>
<li><a href="#carbonwhisper">carbon和whisper合作收集和存储信息</a></li>
<li><a href="#graphite-web">本地安装graphite-web</a><ul>
<li><a href="#django">安装django</a></li>
<li><a href="#graphite-web_1">graphite-web 数据库创建</a></li>
<li><a href="#graphite-web-shell">graphite-web shell调试</a></li>
<li><a href="#shell">第一次shell 测试</a></li>
<li><a href="#graphite-web_2">开启graphite-web</a><ul>
<li><a href="#_10">重要信息</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_11">术语解释</a><ul>
<li><a href="#_12">公网和集群内网</a></li>
<li><a href="#map">各个Map监控信息</a></li>
<li><a href="#cephx_1">cephx认证系统</a></li>
</ul>
</li>
<li><a href="#_13">参考资料</a></li>
</ul>
</div>
        </nav>
    </div>
    <div class="col-md-8 article-content">

            
<h2 id="ceph">ceph简介</h2>
<p>ceph系统是一种分布式存储解决方案，可以通过多台计算机进行配置，搭建ceph集群并提供块设备存储、对象存储和文件系统服务。</p>
<p>ceph项目的设计思想是很先进的，其源于Sage Weil在加州大学攻读博士期间的一篇论文。ceph项目简单来说目的就是应对PB级别的存储需求的，当然ceph在设计上是没有存储理论上限的。</p>
<p>正如ceph官网所言：</p>
<blockquote>
<p>Ceph is a unified, distributed storage system designed for excellent performance, reliability and scalability.</p>
</blockquote>
<p>ceph是一个分布式存储系统，提供了优良的性能，可靠性和可扩展性。可靠性就是ceph存储系统会尽可能地保证数据不会丢失，可扩展性包括系统规模和存储容量的可扩展。此外ceph在运维上还提供了自动化运维特性，包括数据自动replication；自动re-balancing；自动failure detection和自动failure recoery。</p>
<h3 id="ceph_1">ceph存储系统结构</h3>
<p>ceph存储系统如下图所示分为四个层次：</p>
<p><img alt="ceph_system_structure.png" src="https://a358003542.github.io/images/ceph_system_structure.png"/></p>
<ol>
<li>
<p>首先是最底层的RADOS基础存储系统，所有存储在ceph系统的用户数据最终都是由这一层来存储的。理解RADOS是理解ceph系统的关键。</p>
</li>
<li>
<p>基础库librados，这一层对底层RADOS系统进行抽象和封装，并向上提供API。</p>
</li>
<li>
<p>高层应用接口，这一层有RADOS GW、RBD、ceph FS。分别提供对外网关接口，块设备接口和文件系统接口。</p>
</li>
<li>
<p>应用层，基于librados和高层应用接口开发出来的应用方法，比如云对象存储，云硬盘等。</p>
</li>
</ol>
<h3 id="rados">RADOS存储系统结构</h3>
<p>RADOS基础存储系统结构如下图所示：</p>
<p><img alt="rados_system_structure" src="https://a358003542.github.io/images/rados_system_structure.png"/></p>
<p>RADOS系统由很多OSDs组成，其称之为OSD deamon进程，ceph osd daemon 功能是存储数据，处理数据的复制、恢复、回填、再均衡并通过检查其他osd daemon的心跳来向ceph monitor提供一些监控信息。ceph monitor 维护着展示集群状态的各个图表。</p>
<p>PG（Placement Group）在RADOS存储系统中，最终存储的是一个具有最大size的最小对象Object，比如上层的文件最终将分成这样一系列的Object。PG是用来（基于CRUSH算法）组织Object和位置映射的，一个Object只能映射到一个PG，然后Object通过PG映射到多个OSDs上，这里就牵涉到数据分布均匀性和数据修复问题。</p>
<h3 id="ceph_2">ceph存储集群</h3>
<p>整个ceph存储集群的部署就是配置好一个个ceph节点，还有网络，还有ceph存储集群。一个ceph存储集群至少需要一个ceph monitor 和两个ceph osd daemon（OSDs）。如果是ceph文件系统客户端则还需要ceph metadata server（MDS）。</p>
<ul>
<li><code>ceph status</code> 或者 <code>ceph -s</code> ： 查看整个ceph集群的状态</li>
</ul>
<h2 id="_1">安装前的预检</h2>
<ol>
<li>预检操作系统和ceph版本号兼容性</li>
<li>预检硬件，最小的ceph测试环境需要三台机器。也就是官方文档所谓的admin节点【上面执行ceph-deploy操作的节点】和mon节点是在一台机器上。</li>
</ol>
<h3 id="adminceph-deploy">在admin节点安装ceph-deploy</h3>
<p><strong>NOTICE: 下面内容来自ceph中文文档。</strong></p>
<div class="section" id="rpm">
<h3>红帽包管理工具（RPM）</h3>
<p>在 Red Hat （rhel6、rhel7）、CentOS （el6、el7）和 Fedora 19-20 （f19 - f20） 上执行下列步骤：</p>
<ol class="arabic">
<li><p class="first">在 RHEL7 上，用 <tt class="docutils literal"><span class="pre">subscription-manager</span></tt> 注册你的目标机器，确认你的订阅， 并启用安装依赖包的“Extras”软件仓库。例如 ：</p>
<div class="highlight-python"><pre>sudo subscription-manager repos --enable=rhel-7-server-extras-rpms</pre>
</div>
</li>
<li><p class="first">在 RHEL6 上，安装并启用 Extra Packages for Enterprise Linux (EPEL) 软件仓库。 请查阅 <a class="reference external" href="https://fedoraproject.org/wiki/EPEL">EPEL wiki</a> 获取更多信息。</p>
</li>
<li><p class="first">在 CentOS 上，可以执行下列命令：</p>
<div class="highlight-python"><pre>sudo yum install -y yum-utils &amp;&amp; sudo yum-config-manager --add-repo https://dl.fedoraproject.org/pub/epel/7/x86_64/ &amp;&amp; sudo yum install --nogpgcheck -y epel-release &amp;&amp; sudo rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 &amp;&amp; sudo rm /etc/yum.repos.d/dl.fedoraproject.org*</pre>
</div>
</li>
<li><p class="first">把软件包源加入软件仓库。用文本编辑器创建一个 YUM (Yellowdog Updater, Modified) 库文件，其路径为 <tt class="docutils literal"><span class="pre">/etc/yum.repos.d/ceph.repo</span></tt> 。例如：</p>
<div class="highlight-python"><pre>sudo vim /etc/yum.repos.d/ceph.repo</pre>
</div>
<p>把如下内容粘帖进去，用 Ceph 的最新主稳定版名字替换 <tt class="docutils literal"><span class="pre">{ceph-stable-release}</span></tt> （如 <tt class="docutils literal"><span class="pre">firefly</span></tt> ），用你的Linux发行版名字替换 <tt class="docutils literal"><span class="pre">{distro}</span></tt> （如 <tt class="docutils literal"><span class="pre">el6</span></tt> 为 CentOS 6 、 <tt class="docutils literal"><span class="pre">el7</span></tt> 为 CentOS 7 、 <tt class="docutils literal"><span class="pre">rhel6</span></tt> 为 Red Hat 6.5 、 <tt class="docutils literal"><span class="pre">rhel7</span></tt> 为 Red Hat 7 、 <tt class="docutils literal"><span class="pre">fc19</span></tt> 是 Fedora 19 、 <tt class="docutils literal"><span class="pre">fc20</span></tt> 是 Fedora 20 ）。最后保存到 <tt class="docutils literal"><span class="pre">/etc/yum.repos.d/ceph.repo</span></tt> 文件中。</p>
<div class="highlight-python"><pre>[ceph-noarch]
name=Ceph noarch packages
baseurl=http://download.ceph.com/rpm-{ceph-release}/{distro}/noarch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc</pre>
</div>
</li>
<li><p class="first">更新软件库并安装 <tt class="docutils literal"><span class="pre">ceph-deploy</span></tt> ：</p>
<div class="highlight-python"><pre>sudo yum update &amp;&amp; sudo yum install ceph-deploy</pre>
</div>
</li>
</ol>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">你也可以从欧洲镜像 eu.ceph.com 下载软件包，只需把 <tt class="docutils literal"><span class="pre">http://ceph.com/</span></tt> 替换成 <tt class="docutils literal"><span class="pre">http://eu.ceph.com/</span></tt> 即可。</p>
</div>
</div>
<h3 id="ceph_3">关闭各个ceph节点防火墙</h3>
<div class="highlight"><pre><span></span>systemctl stop firewalld
systemctl disable firewalld
systemctl status firewalld
</pre></div>
<h3 id="ceph_4">配置各个ceph节点主机名</h3>
<div class="highlight"><pre><span></span>hostnamectl --static set-hostname {host_name}
</pre></div>
<p>然后是修改各个ceph节点的 <code>/etc/hosts</code> 文件，确保添加了类似如下的内容：</p>
<div class="highlight"><pre><span></span>{public_network_ip_1}  {host_name_1}
{public_network_ip_2}  {host_name_2}
</pre></div>
<h3 id="ntp">配置NTP</h3>
<h4 id="ntp_1">安装NTP服务</h4>
<p>所有节点安装ntp： </p>
<div class="highlight"><pre><span></span>sudo yum -y install ntp ntpdate
</pre></div>
<p>所有节点备份原ntp配置文件：</p>
<div class="highlight"><pre><span></span>cd /etc &amp;&amp; mv ntp.conf ntp.conf.bak
</pre></div>
<p>以ceph1为ntp服务端节点，在ceph1上新建新的NTP配置文件，内容类似如下：</p>
<div class="highlight"><pre><span></span>restrict 127.0.0.1
restrict ::1
restrict 192.168.3.0 mask 255.255.255.0 // ceph1 网段和掩码
server 127.127.1.0
fudge 127.127.1.0 stratum 8
</pre></div>
<p>ceph2和ceph3类似新增NTP配置文件内容类似如下：</p>
<div class="highlight"><pre><span></span>server 192.168.3.166
</pre></div>
<h3 id="ntp_2">NTP服务启动</h3>
<p>ceph1启动ntp服务：</p>
<div class="highlight"><pre><span></span>systemctl start ntpd
systemctl enable ntpd
systemctl status ntpd
</pre></div>
<p>除ceph1之外的其他节点强制与其同步时间：</p>
<div class="highlight"><pre><span></span>ntpdate ceph1
</pre></div>
<p>除ceph1之外的其他节点写入硬件时钟，避免重启失效：</p>
<div class="highlight"><pre><span></span>hwclock -w
</pre></div>
<p>除ceph1之外的其他节点启动crontab并作如下配置：</p>
<div class="highlight"><pre><span></span>yum install -y crontabs
chkconfig crond on
systemctl start crond
</pre></div>
<p>通过 <code>crontab -e</code>  编辑加入如下配置，强制本节点每十分钟和ceph1同步一次时间：</p>
<div class="highlight"><pre><span></span>*/10 * * * * /usr/bin/ntpdate 192.168.3.166
</pre></div>
<h3 id="ceph_5">创建部署ceph的用户</h3>
<p>下面内容复制自ceph中文文档，似乎是一个可选项操作，暂时先放在这里，留作参考。</p>
<div class="section" id="id3">
<h3>创建部署 Ceph 的用户<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p><tt class="docutils literal"><span class="pre">ceph-deploy</span></tt> 工具必须以普通用户登录 Ceph 节点，且此用户拥有无密码使用 <tt class="docutils literal"><span class="pre">sudo</span></tt> 的权限，因为它需要在安装软件及配置文件的过程中，不必输入密码。</p>
<p>较新版的 <tt class="docutils literal"><span class="pre">ceph-deploy</span></tt> 支持用 <tt class="docutils literal"><span class="pre">--username</span></tt> 选项提供可无密码使用 <tt class="docutils literal"><span class="pre">sudo</span></tt> 的用户名（包括 <tt class="docutils literal"><span class="pre">root</span></tt> ，虽然<strong>不建议</strong>这样做）。使用 <tt class="docutils literal"><span class="pre">ceph-deploy</span> <span class="pre">--username</span> <span class="pre">{username}</span></tt> 命令时，指定的用户必须能够通过无密码 SSH 连接到 Ceph 节点，因为 <tt class="docutils literal"><span class="pre">ceph-deploy</span></tt> 中途不会提示输入密码。</p>
<p>我们建议在集群内的<strong>所有</strong> Ceph 节点上给 <tt class="docutils literal"><span class="pre">ceph-deploy</span></tt> 创建一个特定的用户，但<strong>不要</strong>用 “ceph” 这个名字。全集群统一的用户名可简化操作（非必需），然而你应该避免使用知名用户名，因为黑客们会用它做暴力破解（如 <tt class="docutils literal"><span class="pre">root</span></tt> 、 <tt class="docutils literal"><span class="pre">admin</span></tt> 、 <tt class="docutils literal"><span class="pre">{productname}</span></tt> ）。后续步骤描述了如何创建无 <tt class="docutils literal"><span class="pre">sudo</span></tt> 密码的用户，你要用自己取的名字替换 <tt class="docutils literal"><span class="pre">{username}</span></tt> 。</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">从 <a class="reference external" href="../../release-notes/#v9-1-0-infernalis-release-candidate">Infernalis 版</a>起，用户名 “ceph” 保留给了 Ceph 守护进程。如果 Ceph 节点上已经有了 “ceph” 用户，升级前必须先删掉这个用户。</p>
</div>
<ol class="arabic">
<li><p class="first">在各 Ceph 节点创建新用户。</p>
<div class="highlight-python"><pre>ssh user@ceph-server
sudo useradd -d /home/{username} -m {username}
sudo passwd {username}</pre>
</div>
</li>
<li><p class="first">确保各 Ceph 节点上新创建的用户都有 <tt class="docutils literal"><span class="pre">sudo</span></tt> 权限。</p>
<div class="highlight-python"><pre>echo "{username} ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/{username}
sudo chmod 0440 /etc/sudoers.d/{username}</pre>
</div>
</li>
</ol>
</div>
<h3 id="ssh">配置ssh免密登录</h3>
<p>配置ceph1节点对所有主/客户机节点的免密（包括ceph1自身）。</p>
<div class="highlight"><pre><span></span>ssh-keygen -t rsa
for i in {1..3}; do ssh-copy-id ceph$i;done
</pre></div>
<h3 id="selinux">关闭SELinux</h3>
<p>所有节点都需要做，临时关闭：</p>
<div class="highlight"><pre><span></span>setenforce 0
</pre></div>
<p>还需要配置永久关闭，重启后生效，修改 <code>/etc/selinux/config</code>：</p>
<div class="highlight"><pre><span></span>SELINUX=disabled
</pre></div>
<h3 id="epel">配置epel源</h3>
<div class="highlight"><pre><span></span>sudo yum install -y epel-release 
</pre></div>
<h3 id="ceph_6">配置ceph镜像源</h3>
<p>修改 <code>/etc/yum.repos.d/ceph.repo</code> ，并加入如下内容：</p>
<div class="highlight"><pre><span></span>[Ceph]
name=Ceph packages for $basearch
baseurl=http://download.ceph.com/rpm-nautilus/el7/$basearch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
priority=1

[Ceph-noarch]
name=Ceph noarch packages
baseurl=http://download.ceph.com/rpm-nautilus/el7/noarch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
priority=1

[ceph-source]
name=Ceph source packages
baseurl=http://download.ceph.com/rpm-nautilus/el7/SRPMS
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
priority=1
</pre></div>
<p>然后运行：</p>
<div class="highlight"><pre><span></span>sudo yum clean all &amp;&amp; sudo yum makecache
</pre></div>
<h3 id="_2">网络环境预检</h3>
<p><strong>centos7默认使用的是firewalld了，下面的内容主要是关于iptables的，已经过时了，如果防火墙已经关闭了，那么下面的内容可以不用看了，不过看下简单了解下ceph底层的一些网络端口使用情况也好。</strong></p>
<p>首先集群的各个节点机器应该是ping得通的：</p>
<div class="highlight"><pre><span></span>ping 短主机名
</pre></div>
<p>ceph的mon默认监听端口是 <code>6789</code> ，然后其只管public network，其用到的 <code>{iface}</code> 是public network 的interface，其所用到的 <code>{ip-address}</code> 是 public network 的IP address。其所用的netmask是public network的 <code>{netmask}</code> 。</p>
<div class="highlight"><pre><span></span>sudo iptables -A INPUT -i {iface} -p tcp -s {ip-address}/{netmask} --dport 6789 -j ACCEPT
</pre></div>
<p>ceph的osd从当前ceph节点上的6800端口开始找可用端口，默认是<code>6800:7300</code> ，一个osd需要三个端口：</p>
<ul>
<li>一个用于和clients和monitors通信</li>
<li>一个用于和其他osds通信</li>
<li>一个用于心跳通信（heartbeating）</li>
</ul>
<p>这些端口都是限于ceph节点Host机器上的，可以重复，为了防止osd重启端口未正常释放，确保后面还有一些富余的端口号。</p>
<p>公网和私网分离之后，clients会通过公网连接osds，然后各个osds之间会通过私网来连接。这些在iptables那边的rules是分开的。</p>
<div class="highlight"><pre><span></span>sudo iptables -A INPUT -i {iface}  -m multiport -p tcp -s {ip-address}/{netmask} --dports 6800:6810 -j ACCEPT
</pre></div>
<p>请确保上面的6810满足你的osd数量要求。</p>
<p>ceph的mds server监听public network从 6800 开始的第一个可用的端口。类似上面的其也只管public network，其用到的 <code>{iface}</code> 是public network 的interface，其所用到的 <code>{ip-address}</code> 是 public network 的IP address。其所用的netmask是public network的 <code>{netmask}</code> 。</p>
<div class="highlight"><pre><span></span>sudo iptables -A INPUT -i {iface} -m multiport -p tcp -s {ip-address}/{netmask} --dports 6800:6810 -j ACCEPT
</pre></div>
<h2 id="cephconf">ceph.conf的配置</h2>
<p>ceph.conf配置定义了整个ceph集群的一些信息，有：</p>
<ul>
<li>集群id 也就是fsid</li>
<li>认证配置</li>
<li>集群成员</li>
<li>host names</li>
<li>host addresses</li>
<li>keyrings路径</li>
<li>journals路径</li>
<li>data路径</li>
<li>其他runtime配置</li>
</ul>
<p>该文件的查找先后顺序如下所示：</p>
<ol>
<li><code>$CEPH_CONF</code> 环境变量</li>
<li>-c path/path -c命令行参量</li>
<li>/etc/ceph/ceph.conf 系统默认路径</li>
<li>~/.ceph/config 本地用户路径</li>
<li>./ceph.conf 当前工作路径</li>
</ol>
<p>原则上这个配置文件是管理整个集群的所有daemons的，但具体到某个daemon进程上，它又是按照上面的查找顺序限于本机查找的，也就是说，整个ceph集群的配置统一工作，是需要额外的管理的。</p>
<h3 id="_3">具体配置的覆盖</h3>
<p>具体配置的覆盖是 [global] -&gt; [mon] -&gt; [mon.a] ，其他daemon类似。</p>
<h3 id="global">global</h3>
<div class="highlight"><pre><span></span>[global]
public network = {public-network/netmask} # 注意用诸如10.0.0.0/24这样的格式
cluster network = {cluster-network/netmask} # 如果你定义了cluster network，那么osds会把心跳，osds之间的replication recovery通信都走cluster network。
</pre></div>
<h3 id="_4">网络配置最小要求</h3>
<p>网络配置最小要求是ceph的deamon进程在ceph.conf文件里面应该找到 <code>host</code> 变量，这个host变量也就是所谓的hostname也就是所谓的 <code>hostname -s</code> 。然后就是ceph集群已经指明monitor的ip address和端口。</p>
<p>一般具体就是某个daemon section下设置 <code>host</code> ，然后mon或者某个mon section下设置 <code>mon addr</code> 这个变量。</p>
<p>关于daemon具体的 <code>addr</code> 可能是不需要设置的，还不清楚在什么情况下确定可以不设置。</p>
<h2 id="_5">硬盘和文件系统配置</h2>
<p>osd daemon最好在一个单独的硬盘上，如果你一定要在一个硬盘上做多个osd，那么最好是先做分区工作。</p>
<p>生产上推荐是用 <code>XFS</code>  文件系统格式。如果你使用的是 <code>ext4</code> 文件系统格式，那么在你的ceph.conf配置的osd section上推荐加上这么一行：</p>
<div class="highlight"><pre><span></span>filestore xattr use omap = true
</pre></div>
<p>然后ceph官方手册说了一些 <code>btrfs</code> 文件格式的好话，说如果测试的话推荐用这个文件系统格式。</p>
<h2 id="mon">mon的配置</h2>
<p>ceph的mons们维护着一个 master copy of cluster map，也就是ceph client 能够找到所有的ceph monitors ， ceph osd daemons ， ceph mds servers 通过连接一个ceph monitor 然后提出当前的 cluster map。有了这个cluster map还有 CRUSH 算法，ceph client就能够找到所有对象的位置了。</p>
<p>生产环境ceph集群最少应该有三个monitor daemon，测试环境可以只设一个mon。</p>
<p>mon daemon 默认把数据放在 <code>/var/lib/ceph/mon/$cluster-$id</code> 这里。</p>
<h4 id="mon_1">mon的部署基本流程</h4>
<p>除非你用ceph-deploy工具不去考虑具体细节，否则mon的部署都应该如下流程走一遍，其中也包括一些cephx的认证配置的东西。</p>
<p>出于简洁性的考虑下面的步骤将会把对 ceph.conf 的一些配置更改操作步骤略去了，请读者自行补充。</p>
<ol>
<li>为你的集群创建一个 keyring 并为你的 mon 生成一个secret key</li>
</ol>
<p><code>ceph-authtool --create-keyring /tmp/ceph.mon.keyring --gen-key -n mon. --cap mon 'allow *'</code></p>
<ol>
<li>生成一个administrator keyring 生成 client.admin 用户，并把这个用户加入到 keyring 中。</li>
</ol>
<p><code>ceph-authtool --create-keyring /etc/ceph/ceph.client.admin.keyring --gen-key -n client.admin --set-uid=0 --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow'</code></p>
<ol>
<li>把 client.admin 的key添加到 <code>ceph.mon.keyring</code> 里面</li>
</ol>
<p><code>ceph-authtool /tmp/ceph.mon.keyring --import-keyring /etc/ceph/ceph.client.admin.keyring</code></p>
<ol>
<li>生成mon map，将其保存到 <code>/tmp/monmap</code> 你至少需要一个 hostname 和 ip-address 参数，还有fsid参数，还有集群名字否则用默认的 <code>ceph</code> 。</li>
</ol>
<p><code>monmaptool --create --add {hostname} {ip-address} --fsid {uuid} /tmp/monmap</code></p>
<ol>
<li>确保你的mon data 文件夹是存在的</li>
</ol>
<p><code>sudo mkdir /var/lib/ceph/mon/{cluster-name}-{hostname}</code></p>
<ol>
<li>把上面的 mon map 还有keyring添加进来</li>
</ol>
<p><code>ceph-mon --mkfs -i {hostname} --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring</code></p>
<ol>
<li>创建一个空白 <code>done</code> 文件</li>
</ol>
<p><code>sudo touch /var/lib/ceph/mon/ceph-node1/done</code></p>
<ol>
<li>启动mon</li>
</ol>
<p><code>sudo /etc/init.d/ceph start mon.node1</code></p>
<h3 id="mon_2">mon之间的同步</h3>
<p>当有多个mon的时候，mon之间会彼此看看谁有最新的cluster map，然后取出最新的cluster信息。在同步过程中，mon之间会出现三种角色：</p>
<ol>
<li>Leader Leader第一个拥有最新的cluster map</li>
<li>Provider Provider有最新的cluster map，不过不是第一个有的。</li>
<li>Requester Requester的cluster map有点掉队了，需要进行同步操作。</li>
</ol>
<h2 id="osd">osd的配置</h2>
<p>一个osd实际上就是一个ceph-osd deamon和Guest机上的一个存储点，如果一个Guest机上你设置了多个存储点，只需要一个ceph-osd deamon就可以了。</p>
<p>生产环境一般是一个节点（Guest机器）一个osd daemon，一个存储盘。</p>
<p>设置日志尺寸如下所示：</p>
<div class="highlight"><pre><span></span>[osd]
osd journal size = 10000
</pre></div>
<p>osd daemon 默认把数据放在 <code>/var/lib/ceph/osd/$cluster-$id</code> 这里。你可以通过 <code>osd data</code> 改变这个值，推荐系统盘和osd盘分开，然后osd盘作为挂载点使用你需要先将其挂载好。</p>
<div class="highlight"><pre><span></span>sudo mkfs -t {fstype} /dev/{disk}
sudo mount -o user_xattr /dev/{hdd} /var/lib/ceph/osd/ceph-{osd-number}
</pre></div>
<h3 id="osd_1">osd的心跳</h3>
<p>osd之间会彼此检查心跳（每6秒），这个心跳周期可以通过参数 <code>osd heartbeat interval</code>  来设置。如果一个osd deamon 20秒内没有返回心跳信息，那么就会被认为这个osd deamon <code>down</code> 了。这个时间可以通过 <code>osd heartbeat grace</code> 来设置。mon那边需要接受某个osd deamon <code>down</code> 的信息 <code>三次</code> 才会认为这个osd deamon已经down了。这个次数可以通过 <code>mon osd min down reports</code> ，默认一个osd报告者就可以了，你可以通过 <code>mon osd min down reporters</code> 来设置要求的报告者数目。</p>
<p>上面的是osd之间彼此沟通心跳，如果一个    osd没办法和任何其他的osd进行心跳沟通，那么每过30秒其会向mon请求最新的cluster map，这个时间由 <code>osd mon heartbeat interval</code>  设置。</p>
<p>然后是mon那边，如果一个osd deamon 一定时间内没有向mon进行报告，则会被认为是down了。具体有很多报告事件，这个后面再详细讨论。</p>
<h3 id="osd_2">osd的日志</h3>
<p>日志的大小至少是 ( drive speed * filestore max sync interval ) * 2 ，不过最常见的实践是给日志文件专门弄个分区（SSD），然后把整个分区都挂载为专门用做存储日志。</p>
<p>日志设置为 <code>osd journal</code> 默认路径是： <code>/var/lib/ceph/osd/$cluster-$id/journal</code> </p>
<p>日志大小设置为 <code>osd journal size</code> 默认大小是： <code>0</code> ... ， 所以你想要日志就需要设置这个值。</p>
<div class="highlight"><pre><span></span>osd journal size = {2 * (expected throughput * filestore max sync interval)}
</pre></div>
<p>上面期待的吞吐量取硬盘吞吐量或网络吞吐量的最小值。</p>
<h2 id="_6">认证配置</h2>
<p>认证配置应该都是全局的放在global section哪里。</p>
<p>认证配置默认是开启 <code>cephx</code> 认证。生产环境肯定是要开启来的。测试并没有这个要求，但最好也开启，并习惯有这个认证配置的存在。</p>
<h4 id="cephx">开启cephx</h4>
<ol>
<li>创建一个 client.admin 的keyring</li>
</ol>
<p><code>ceph auth get-or-create client.admin mon 'allow *' mds 'allow *' osd 'allow *' -o /etc/ceph/ceph.client.admin.keyring</code></p>
<p>这一步只要目标点 <code>/etc/ceph/ceph.client.admin.keyring</code> 已经有了那么就不应该做了。</p>
<ol>
<li>为你的monitor集群创建monitor secret key，这一部分在前面mon的基本配置中也看到过。</li>
</ol>
<p><code>ceph-authtool --create-keyring /tmp/ceph.mon.keyring --gen-key -n mon. --cap mon 'allow *'</code></p>
<ol>
<li>
<p>把上面创建的 monitor secret key复制到每个mon的mon data哪里去。</p>
</li>
<li>
<p>为每个osd生成一个secret key</p>
</li>
</ol>
<p><code>ceph auth get-or-create osd.{$id} mon 'allow rwx' osd 'allow *' -o /var/lib/ceph/osd/ceph-{$id}/keyring</code></p>
<ol>
<li>为每个mds生成一个secret key</li>
</ol>
<p><code>ceph auth get-or-create mds.{$id} mon 'allow rwx' osd 'allow *' mds 'allow *' -o /var/lib/ceph/mds/ceph-{$id}/keyring</code></p>
<h3 id="_7">术语</h3>
<ul>
<li>FQDN fqdn Full Qualified Domain Name 全称域名，全称域名包含两部分：hostname和domain name。目前fqdn和hostname都是node001...之类的，但ceph-ansible的代码里面有些地方会区分。</li>
<li>hostname 这是最常见的一个概念，从vagrant起在各个机器里面就已经确定了，说hostname或者说 <code>hostname -s</code> 或者说 <code>host</code> 通常都是指 node001... 之类的。目前各个虚拟机hostname也是可以自己ping得通的，可能有些配置的hostname只是一个名字，有的会有网络含义，不管怎么说，以后到物理机，首先请确保目标物理机的hostname可以ping得通，免得引起不必要的麻烦。</li>
</ul>
<h3 id="metavariable">METAVARIABLE</h3>
<p>在ceph.conf里面有些特殊的变量，ceph会自动填充它们给与它们合适的值。</p>
<ul>
<li><code>$cluster</code>  扩展为ceph集群的名字，默认为 <code>ceph</code></li>
<li><code>$type</code>  扩展为 mds 或者 osd 或者 mon ，具体取决于是当前那个daemon进程在使用这个配置文件。</li>
<li><code>$id</code>  扩展为当前daemon的identifier，比如说 <code>osd.0</code> ，该值就是 <code>0</code> 。</li>
<li><code>$host</code>  扩展为当前daemon进程的host变量值，这个前面说过了每个daemon进程在ceph.conf里面都应该找到其对应的host变量</li>
<li><code>$name</code> 等价于 <code>$type.$id</code> </li>
</ul>
<h3 id="_8">运行时更改配置</h3>
<div class="highlight"><pre><span></span>ceph tell {daemon-type}.{id or *} injectargs --{name} {value} [--{name} {value}]
</pre></div>
<p>具体配置名有可能需要加上 <code>_</code> 或 <code>-</code> 。</p>
<h3 id="_9">运行时查看配置</h3>
<div class="highlight"><pre><span></span>ceph daemon {daemon-type}.{id} config show | less
</pre></div>
<h2 id="diamond">安装diamond模块</h2>
<p><a href="https://github.com/python-diamond/Diamond">https://github.com/python-diamond/Diamond</a></p>
<p>diamond是用来收集信息的，其发出的信息格式如下：</p>
<div class="highlight"><pre><span></span>servers.wanze-ubuntu.iostat.sda6.writes_merged 60.000 1469329380
</pre></div>
<div class="highlight"><pre><span></span>什么path 值是多少 时间戳
</pre></div>
<p>这种格式具有通用性，后面carbon部分就是接受这种信息格式，甚至后面自己写额外的数据发送也应该按照这种格式来。 </p>
<p>diamond依赖 <code>configobj</code> 和 <code>psutil</code> 这两个模块。其中configobj似乎还依赖six模块，而psutil安装则需要花费一点功夫，首先需要安装 gcc</p>
<div class="highlight"><pre><span></span>yum install gcc
</pre></div>
<p>然后是 python-devel </p>
<div class="highlight"><pre><span></span>yum install python-devel
</pre></div>
<p>diamond这里安装的是4.0.451，安装同上，然后这个模块安装后会在虚拟环境下创建一些额外功能的文件夹和文件，这个下面详细说明之。</p>
<ol>
<li>bin/diamond 虚拟环境下的可执行脚本，后面类似的不多说了。</li>
<li>etc/diamond 这里放着diamond的配置，后续python模块的配置都推荐类似放在这里。</li>
<li>share/diamond 这里放着diamond的一些资源</li>
</ol>
<p>类似的虚拟环境文件夹设置有：</p>
<ul>
<li>var/run 下面放着pid文件</li>
<li>var/log 下面放着日志文件</li>
<li>storage/ 等下carbon和graphite-web都会往里面塞一些内容，作为内容存储的地方。</li>
</ul>
<p>现在我们虚拟环境下运行 <code>diamond</code> 就可以看到这个命令了，只是需要指定配置文件在哪里。</p>
<p>将/opt/sdsom/venv/etc/diamond 下的 diamond.conf.example 改名为 diamond.conf （现在先这样编写，后面安装时可以考虑批量往etc等地方塞配置文件。）</p>
<div class="highlight"><pre><span></span># Pid file
pid_file = /opt/sdsom/venv/var/run/diamond.pid

# Directory to load collector modules from
collectors_path = /opt/sdsom/venv/share/diamond/collectors/

# Directory to load collector configs from
collectors_config_path = /opt/sdsom/venv/etc/diamond/collectors/

# Directory to load handler configs from
handlers_config_path = /opt/sdsom/venv/etc/diamond/handlers/

# Directory to load handler modules from
handlers_path = /opt/sdsom/venv/share/diamond/handlers/
</pre></div>
<p>原则上所有的这些文件都应该放入 <code>/opt/sdsom/venv</code> 里面，现在先暂时这样写死了，后面应该考虑配置文件自动基于模块填充的生成机制。</p>
<div class="highlight"><pre><span></span>### Defaults options for all Handlers
[[default]]

[[ArchiveHandler]]

# File to write archive log files
log_file = /opt/sdsom/venv/var/log/diamond/archive.log
</pre></div>
<p>这里的host变量是sdsom模块后面怎么修改了的，现在测试简单填写为127.0.0.1即可。这里的port将和后面的carbon直接对应。</p>
<div class="highlight"><pre><span></span>[[GraphiteHandler]]
### Options for GraphiteHandler

# Graphite server host
host = 127.0.0.1

# Port to send metrics to
port = 6601
</pre></div>
<div class="highlight"><pre><span></span>[[GraphitePickleHandler]]
### Options for GraphitePickleHandler

# Graphite server host
host = 127.0.0.1

# Port to send metrics to
port = 6602
</pre></div>
<p>默认的这些信息收集器是开启的：</p>
<div class="highlight"><pre><span></span>[[CPUCollector]]
enabled = True

[[DiskSpaceCollector]]
enabled = True

[[DiskUsageCollector]]
enabled = True

[[LoadAverageCollector]]
enabled = True

[[MemoryCollector]]
enabled = True

[[VMStatCollector]]
enabled = True
</pre></div>
<p>先暂时留一两个方便测试用即可。</p>
<div class="highlight"><pre><span></span>[handler_rotated_file]
</pre></div>
<div class="highlight"><pre><span></span>class = handlers.TimedRotatingFileHandler
level = DEBUG
formatter = default
# rotate at midnight, each day and keep 7 days
args = ('/opt/sdsom/venv/var/log/diamond/diamond.log', 'midnight', 1, 7)
</pre></div>
<p>有些文件夹需要手工创建下：</p>
<div class="highlight"><pre><span></span>mkdir -p /opt/sdsom/venv/var/log/diamond
mkdir -p /opt/sdsom/venv/var/run
</pre></div>
<p>配置完了之后，如下开启一个diamond进程，后续可以考虑用supervisor来管理开启。</p>
<div class="highlight"><pre><span></span>diamond -c /opt/sdsom/venv/etc/diamond/diamond.conf 
</pre></div>
<p>然后我们可以去看下创建的pid文件已经相应的日志文件。关闭该diamond进程就是kill掉该进程号即可。 </p>
<h2 id="carbonwhisper">carbon和whisper合作收集和存储信息</h2>
<p><strong>注意：</strong> carbon在虚拟环境下的安装需要额外安装参数配置。</p>
<div class="highlight"><pre><span></span>pip install --install-option="--prefix=/opt/sdsom/venv" --install-option="--install-lib=/opt/sdsom/venv/lib/${PYTHON_VESRSION}/site-packages" carbon-0.9.15.tar.gz
</pre></div>
<p>其依赖于 Twisted ，先把它安装上去。注意最新版本的twisted已经不支持python2.6了。（其依赖于zope.interface，不是很大直接从网络上安装了）</p>
<p>其依赖于 txAMQP-0.6.2.tar.gz 直接从网络上安装了。</p>
<p>其额外创建了 storage 文件夹前面已经提及。其额外创建的conf文件夹，里面是一些配置文件，后续建议都统一到 etc/carbon/ 文件夹里面去。</p>
<p>examples 文件夹还不清楚有啥用。</p>
<p>bin里面多了一些python可执行脚本文件，其中 <code>carbon-cache</code> 是直接和whisper交互的，然后aggregator和relay是高级功能，后续可以慢慢了解。</p>
<div class="highlight"><pre><span></span>validate-storage-schemas.py
carbon-cache.py
carbon-aggregator.py
carbon-client.py
carbon-relay.py
</pre></div>
<p>然后还需要安装 whisper 模块</p>
<p>然后在虚拟环境文件夹下的那个conf文件夹里面有个carbon.conf.example，将其改名为carbon.conf，稍微做如下修改：</p>
<div class="highlight"><pre><span></span>GRAPHITE_ROOT = /opt/sdsom/venv/
LOG_DIR = /opt/sdsom/venv/var/log/carbon/
PID_DIR = /opt/sdsom/venv/var/run
</pre></div>
<p>类似的还需要创建一个 storage-schemas.conf 文件。</p>
<p>启动carbon-cache进程如下执行：</p>
<div class="highlight"><pre><span></span>carbon-cache.py start 
</pre></div>
<p>此外还有 status stop 子命令。 然后我们可以去storage那边去看下具体的输出wsp文件。</p>
<h2 id="graphite-web">本地安装graphite-web</h2>
<div class="highlight"><pre><span></span>pip install --install-option="--prefix=/opt/sdsom/venv" --install-option="--install-lib=/opt/sdsom/venv/webapp" graphite-web-0.9.15.tar.gz 
</pre></div>
<p>webapp那边实际上就是 graphite-web 的一个django app。</p>
<h3 id="django">安装django</h3>
<p>由于目前sdsom代码是基于django（1.5.1）此外graphite-web依赖于 django-tagging (0.3.1) ，这个最好先限定版本号。</p>
<h3 id="graphite-web_1">graphite-web 数据库创建</h3>
<div class="highlight"><pre><span></span>django-admin.py syncdb --pythonpath ./webapp  --settings graphite.settings 
</pre></div>
<h3 id="graphite-web-shell">graphite-web shell调试</h3>
<div class="highlight"><pre><span></span>django-admin.py shell --pythonpath ./webapp  --settings graphite.settings 
</pre></div>
<h3 id="shell">第一次shell 测试</h3>
<p>这里新引入一个依赖包 pytz </p>
<div class="highlight"><pre><span></span>from graphite.render.attime import parseATTime
from graphite.render.datalib import fetchData
from django.conf import settings
import pytz
tzinfo = pytz.timezone(settings.TIME_ZONE)
until_time = parseATTime('now', tzinfo)

series = fetchData({'startTime': parseATTime('-30min', tzinfo),'endTime':until_time,'now':until_time,'localOnly':True},'servers.localhost.iostat.sda2.iops')

res = [i for i in series[0]]

&gt;&gt;&gt; res
[None, None, 2.727, None, None, None, None, 1.24, None, None, None, None, 2.63, None, None, None, None, 1.097, None, None, None, None, 1.077, None, None]
</pre></div>
<h3 id="graphite-web_2">开启graphite-web</h3>
<div class="highlight"><pre><span></span>django-admin.py runserver --pythonpath ./webapp --settings graphite.settings 0.0.0.0:8080
</pre></div>
<h4 id="_10">重要信息</h4>
<ol>
<li>
<p>graphite-web 里面的 local_settings 里面可能有一些参数很重要，需要设置，其会通过graphite模块里面加载settings时自动加载。</p>
</li>
<li>
<p>graphite-web 外网网页实际上现在就可以看了，但是要正常显示图片还需要安装cairocffi这个模块，这个模块安装起来依赖的东西就多了（主要是libffi-devel这个系统包等），而就目前的项目架构来说是没有必要的。</p>
</li>
<li>
<p>graphite-web 官方文档推荐的做法是通过其提供的render api来获取数据，其和这里的 fetchData 的主要区别有，直接fetch少了缓冲cache层，然后还少了其他一些不太重要的包装。</p>
</li>
<li>
<p>如果仅仅只是利用fetchData或其中的某个模块，那么后续可以考虑安装参数改为：</p>
<p>pip install --install-option="--prefix=/opt/sdsom/venv" --install-option="--install-lib=/opt/sdsom/venv/lib/python2.7/site-packages" graphite-web-0.9.15.tar.gz </p>
</li>
</ol>
<p>将这个graphite直接作为模块安装进去。</p>
<p>还有如果仅仅只是利用fetchData函数，最多考虑后续加上Cache层，没必要用apache服务起来，也没有必要设置wsgi或者和sdsom等后面的django框架集成在一起。实际上graphite-web这个模块的django层可以考虑完全抽离出去，只是如果懒得做的话就这样了。</p>
<h2 id="_11">术语解释</h2>
<ul>
<li>rados lspools ： 列出pool情况</li>
<li>ceph osd tree : 列出osd的情况</li>
<li>ceph metadata server 为ceph文件系统存储元数据。元数据服务器使得POSIX文件系统的用户在不对Ceph集群造成负担的前提下，执行诸如ls、find等基本命令。</li>
<li>RBD rados block device 是ceph对外提供块设备服务</li>
<li>MDS ceph文件系统依赖的元数据服务</li>
<li>CRUSH ceph使用的数据分布算法</li>
<li>唯一标识符 Unique identifier 也就是 <code>fsid</code> 是集群的唯一标识，你可以通过 <code>uuidgen</code> 命令来生成一个fsid，然后写入ceph的配置文件中，默认是 <code>/etc/ceph/ceph.conf</code> 。</li>
<li>集群名字 Cluster Name 每个ceph集群都有自己的名字，默认是 ceph </li>
<li>监视器名字 Monitor Name 每个集群里面的各个监视器</li>
</ul>
<h3 id="_12">公网和集群内网</h3>
<p>ceph默认你只有一个网络，也就是public network，不过推荐的设置是有两个网卡（这将会带来性能极大的提升），一个网卡： public network， 一个网卡： cluster network。cluster network最好不能连外网，这样会更加安全点。而且cluster网卡最好速度很快，这样各个osd之间通信传输数据才会高效，10Gbps是推荐。</p>
<p>cluster network主要是给osds通信用，所以ceph的其他组件其实还是都依赖于public network的。</p>
<h3 id="map">各个Map监控信息</h3>
<ul>
<li>
<p>Montior Map： 包含集群的 fsid 、位置、名字、地址和端口，也包括当前版本、创建时间、最近修改时间。要查看监视器图，用 <code>ceph mon dump</code> 命令。</p>
</li>
<li>
<p>OSD Map： 包含集群 fsid 、创建时间、最近修改时间、存储池列表、副本数量、归置组数量、OSD 列表及其状态（如 up 、 in ）。要查看OSD运行图，用 <code>ceph osd dump</code> 命令。</p>
</li>
<li>
<p>PG Map： 包含归置组版本、其时间戳、最新的 OSD 运行图版本、占满率、以及各归置组详情，像归置组 ID 、 up set 、 acting set 、 PG 状态（如 active+clean ），和各存储池的数据使用情况统计。</p>
</li>
<li>
<p>CRUSH Map： 包含存储设备列表、故障域树状结构（如设备、主机、机架、行、房间、等等）、和存储数据时如何利用此树状结构的规则。要查看 CRUSH 规则，执行 <code>ceph osd getcrushmap -o {filename}</code> 命令；然后用 <code>crushtool -d {comp-crushmap-filename} -o {decomp-crushmap-filename}</code> 反编译；然后就可以用 cat 或编辑器查看了。</p>
</li>
<li>
<p>MDS Map： 包含当前 MDS 图的版本、创建时间、最近修改时间，还包含了存储元数据的存储池、元数据服务器列表、还有哪些元数据服务器是 up 且 in 的。要查看 MDS 图，执行 <code>ceph mds dump</code> 。</p>
</li>
</ul>
<h3 id="cephx_1">cephx认证系统</h3>
<p>ceph用cephx认证系统来认证用户和守护进程。</p>
<p>Cephx 用共享密钥来认证，即客户端和监视器集群各自都有客户端密钥的副本。这样的认证协议使参与双方不用展现密钥就能相互认证，就是说集群确信用户拥有密钥、而且用户相信集群有密钥的副本。</p>
<p>要使用 cephx ，管理员必须先设置好用户。在下面的图解里， client.admin 用户从命令行调用来生成一个用户及其密钥， Ceph 的认证子系统生成了用户名和密钥、副本存到监视器然后把此用户的密钥回传给 client.admin 用户，也就是说客户端和监视器共享着相同的密钥。</p>
<h2 id="_13">参考资料</h2>
<ol>
<li><a href="https://tobegit3hub1.gitbooks.io/ceph_from_scratch/content/index.html">ceph from scratch</a></li>
<li><a href="http://www.csdn.net/article/2014-04-01/2819090-ceph-swift-on-openstack">ceph浅析上</a></li>
<li><a href="http://www.csdn.net/article/2014-04-08/2819192-ceph-swift-on-openstack-m">ceph浅析中</a></li>
<li><a href="http://docs.ceph.org.cn/">ceph中文文档</a></li>
<li><a href="https://support.huaweicloud.com/productdesc-kunpengsdss/kunpengsdss_01_0001.html">鲲鹏分布式存储解决方案参考文档</a></li>
</ol>

            
            <hr/>

        </div>
        <section>
        <div class="col-md-2" style="float:right;font-size:0.9em;">
            <h4>首发于：</h4>
            <time pubdate="pubdate" datetime="2020-05-24T16:53:27.929151+08:00">2020年 5月 24日 </time>

            <h4>分类：</h4>
            <a class="category-link" href="https://a358003542.github.io/categories.html#others-ref">others</a>

        </div>
        </section>
</div>
</article>
    </div>
    <div class="col-md-1"></div>

</div>


<div id="push"></div>
<button id="gotop" type="button" class="btn btn-default">
    <span class="glyphicon glyphicon-arrow-up" aria-hidden="true"></span>
</button>

<footer>
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-power">Powered by <a href="http://getpelican.com/" title="Pelican Home Page">Pelican</a>. Theme: <a href="http://oncrashreboot.com/pelican-elegant" title="Theme Elegant Home Page">Elegant</a> by <a href="http://oncrashreboot.com" title="Talha Mansoor Home Page">Talha Mansoor</a> and updated by <a href="https://github.com/a358003542" title="wanze Home Page">wanze</a></li>
    </ul>
</div>
</footer>

        <script src="https://a358003542.github.io/theme/js/jquery.min.js"></script>
    <script src="https://a358003542.github.io/theme/js/bootstrap.min.js"></script>

    <script src="https://a358003542.github.io/theme/js/base.js"></script>

    


</body>
</html>