<!DOCTYPE html>
<html lang="zh-cn">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="baidu-site-verification" content="D4VqC4HppC"/>
    <meta name="msvalidate.01" content="55CB117A61A6F8286173763FB18D9625"/>

        <meta name="author" content="cdwanze"/>
        <meta name="copyright" content="cdwanze"/>

        <meta property="og:type" content="article"/>
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="dask, draft, " />

<meta property="og:title" content="dask模块 "/>
<meta property="og:url" content="https://docs.cdwanze.work/drafts/dask-module.html" />
<meta property="og:description" content="前言 总的说来对于python生态圈来说，大数据解决方案我是推荐dask多于pyspark。接触pyspark之后最大的痛苦就是，之前学习机器学习积累下来的numpy，pandas，sklearn的知识，全部不能继续使用了。而dask作为python的大数据解决方案，不说完全无缝对接，这也是不现实的，毕竟到了大数据那块，不说其他东西，就是你要应对的变量，也必然是惰性加载和惰性求值的，这也是一个区别点。但总的给我的感觉就是，作为python爱好者，之于大数据解决方案，dask真的用的很爽。 dask的官方文档写的很厚实，主要是这块东西也多。本文作者尽我所知简要地说下个人使用经验吧。其他深入研究还是要读者去研究 官方文档 的。 大数据hadoop简介 现在的hadoop2的架构大概如下所示： 底层是hadoop分布式文件系统作为大数据的文件存储支持。HDFS的基本架构如下图所示： 然后中层是YARN对计算资源进行调度分配。YARN的基本架构如下图所示： 如上图所示，Yarn有一个Scheduler，也就是Resource Manager 进行资源调度，然后还有很多 个worker，也就是Node Manager进行实际的应用计算任务。 而最高层MapReduce框架是分布式应用程序的一个支持性框架，现在Hadoop2可扩展性很好，dask，spark等这些框架都是可以运行在Yarn之上的。 dask简介 dask主要由两部分组成： 基于计算优化的动态调度 大数据级别的集合 简单来说就是dask基于python常见的那些数据类型概念 …" />
<meta property="og:site_name" content="cdwanze的博文" />
<meta property="og:article:author" content="cdwanze" />
<meta property="og:article:published_time" content="2019-05-06T00:00:00+08:00" />
<meta property="" content="2019-05-06T00:00:00+08:00" />
<meta name="twitter:title" content="dask模块 ">
<meta name="twitter:description" content="前言 总的说来对于python生态圈来说，大数据解决方案我是推荐dask多于pyspark。接触pyspark之后最大的痛苦就是，之前学习机器学习积累下来的numpy，pandas，sklearn的知识，全部不能继续使用了。而dask作为python的大数据解决方案，不说完全无缝对接，这也是不现实的，毕竟到了大数据那块，不说其他东西，就是你要应对的变量，也必然是惰性加载和惰性求值的，这也是一个区别点。但总的给我的感觉就是，作为python爱好者，之于大数据解决方案，dask真的用的很爽。 dask的官方文档写的很厚实，主要是这块东西也多。本文作者尽我所知简要地说下个人使用经验吧。其他深入研究还是要读者去研究 官方文档 的。 大数据hadoop简介 现在的hadoop2的架构大概如下所示： 底层是hadoop分布式文件系统作为大数据的文件存储支持。HDFS的基本架构如下图所示： 然后中层是YARN对计算资源进行调度分配。YARN的基本架构如下图所示： 如上图所示，Yarn有一个Scheduler，也就是Resource Manager 进行资源调度，然后还有很多 个worker，也就是Node Manager进行实际的应用计算任务。 而最高层MapReduce框架是分布式应用程序的一个支持性框架，现在Hadoop2可扩展性很好，dask，spark等这些框架都是可以运行在Yarn之上的。 dask简介 dask主要由两部分组成： 基于计算优化的动态调度 大数据级别的集合 简单来说就是dask基于python常见的那些数据类型概念 …">

    <title>
dask模块  · cdwanze的博文
</title>


        <link href="https://docs.cdwanze.work/theme/css/font-awesome.css" rel="stylesheet"
              media="screen">
        <link href="https://docs.cdwanze.work/theme/css/bootstrap.min.css" rel="stylesheet"
              media="screen">

            <link rel="stylesheet" type="text/css"
                  href="https://docs.cdwanze.work/theme/css/pygments.css" media="screen">
            <link rel="stylesheet" type="text/css"
                  href="https://docs.cdwanze.work/theme/css/elegant.css" media="screen">
            <link rel="stylesheet" type="text/css"
                  href="https://docs.cdwanze.work/theme/css/custom.css" media="screen">






</head>
<body>

<nav class="navbar">
    <div class="navbar navbar-default" role="navigation">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed"
                    data-toggle="collapse" data-target=".navbar-collapse"
                    aria-expanded="false">
                <span class="sr-only">Navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>

            <a class="navbar-brand" href="https://docs.cdwanze.work/"><span
                    class=site-name>网站首页</span></a>
        </div>


        <div class="navbar-collapse collapse">
            <form action="https://docs.cdwanze.work/search.html"
                  onsubmit="return validateForm(this.elements['q'].value);"
                  class="navbar-form navbar-left">
                <div class="form-group">
                    <input type="text" name="q" id="tipue_search_input"
                           class="form-control" placeholder="Search..."
                           style="width:430px;">
                </div>
                <button class="btn btn-default" type="submit">搜索</button>
            </form>


            <ul class="nav navbar-nav nav-pills navbar-right">
                <li >
                    <a href="https://docs.cdwanze.work">博文首页</a></li>

                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle"
                           data-toggle="dropdown" role="button"
                           aria-haspopup="true" aria-expanded="false">查找文章<span
                                class="caret"></span></a>
                        <ul class="dropdown-menu">
                            <li><a class="slowscroll" href="/categories.html">按分类</a>
                            </li>
                            <li><a class="slowscroll" href="/tags.html">按标签</a>
                            </li>
                        </ul>
                    </li>


                        <li >
                            <a href="https://docs.cdwanze.work/pdf-ebooks.html">pdf电子书籍</a>
                        </li>
                        <li >
                            <a href="https://docs.cdwanze.work/epub-ebooks.html">epub电子书籍</a>
                        </li>
                        <li >
                            <a href="https://docs.cdwanze.work/about.html">关于本网站</a>
                        </li>
            </ul>


        </div>
    </div>
</nav>


<div class="container-fluid">
    <div class="col-md-1 col-md-1-left"></div>
    <div class="col-md-10">
<article>
<div class="row">
    <header class="page-header col-md-10 col-md-offset-2">
    <h1><a href="https://docs.cdwanze.work/drafts/dask-module.html"> dask模块  </a></h1>
    </header>
</div>

<div class="row">
    <div class="col-md-2 table-of-content">
        <nav>
        <h4>目录</h4>
        <div class="toc">
<ul>
<li><a href="#_1">前言</a></li>
<li><a href="#hadoop">大数据hadoop简介</a></li>
<li><a href="#dask">dask简介</a></li>
<li><a href="#dask-yarn">dask-yarn</a><ul>
<li><a href="#worker">实际调整worker资源</a></li>
<li><a href="#_2">关闭应用释放资源</a></li>
<li><a href="#_3">强制关闭某个应用</a></li>
</ul>
</li>
<li><a href="#dataframe">dataframe最佳实践</a></li>
<li><a href="#dataframemap_partitions">dataframe的map_partitions 返回值</a></li>
<li><a href="#dask_1">dask单机版</a></li>
<li><a href="#_4">参考资料</a></li>
</ul>
</div>
        </nav>
    </div>
    <div class="col-md-8 article-content">

            
            
<h2 id="_1">前言</h2>
<p>总的说来对于python生态圈来说，大数据解决方案我是推荐dask多于pyspark。接触pyspark之后最大的痛苦就是，之前学习机器学习积累下来的numpy，pandas，sklearn的知识，全部不能继续使用了。而dask作为python的大数据解决方案，不说完全无缝对接，这也是不现实的，毕竟到了大数据那块，不说其他东西，就是你要应对的变量，也必然是惰性加载和惰性求值的，这也是一个区别点。但总的给我的感觉就是，作为python爱好者，之于大数据解决方案，dask真的用的很爽。</p>
<p>dask的官方文档写的很厚实，主要是这块东西也多。本文作者尽我所知简要地说下个人使用经验吧。其他深入研究还是要读者去研究 <a href="https://docs.dask.org/en/latest/">官方文档</a> 的。</p>
<h2 id="hadoop">大数据hadoop简介</h2>
<p>现在的hadoop2的架构大概如下所示：</p>
<p><img alt="img" src="https://docs.cdwanze.work/images/大数据/HADOOP.png"/></p>
<p>底层是hadoop分布式文件系统作为大数据的文件存储支持。HDFS的基本架构如下图所示：</p>
<p><img alt="img" src="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/images/hdfsarchitecture.png"/></p>
<p>然后中层是YARN对计算资源进行调度分配。YARN的基本架构如下图所示：</p>
<p><img alt="img" src="http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/yarn_architecture.gif"/></p>
<p>如上图所示，Yarn有一个Scheduler，也就是Resource Manager 进行资源调度，然后还有很多 个worker，也就是Node Manager进行实际的应用计算任务。</p>
<p>而最高层MapReduce框架是分布式应用程序的一个支持性框架，现在Hadoop2可扩展性很好，dask，spark等这些框架都是可以运行在Yarn之上的。</p>
<h2 id="dask">dask简介</h2>
<p>dask主要由两部分组成：</p>
<ol>
<li>基于计算优化的动态调度 </li>
<li>大数据级别的集合</li>
</ol>
<p>简单来说就是dask基于python常见的那些数据类型概念，比如array，dataframe，进行大数据级别的集合扩展，其有很多个worker和一个scheduler，调度器主要负责计算调度，调度器面临的最小计算单位是所谓的partition，这个partition会分散在不同的worker中，具体这些worker会利用相同的python环境，来进行某些计算。</p>
<p>dask的安装就python模块来说不是难点所在，这一块难点主要在大数据环境的搭建上。如果你们大数据工作组已经将环境搭建好了，那么利用dask-yarn还是很容易进行dask的分布式任务的分发工作的。</p>
<h2 id="dask-yarn">dask-yarn</h2>
<p>dask连接yarn集群</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">dask_yarn</span> <span class="kn">import</span> <span class="n">YarnCluster</span>
<span class="kn">from</span> <span class="nn">dask.distributed</span> <span class="kn">import</span> <span class="n">Client</span>

<span class="c1"># Create a cluster where each worker has two cores and eight GiB of memory</span>
<span class="n">cluster</span> <span class="o">=</span> <span class="n">YarnCluster</span><span class="p">(</span><span class="n">environment</span> <span class="o">=</span> <span class="s1">'environment.tar.gz'</span><span class="p">,</span>
                      <span class="n">worker_vcores</span><span class="o">=</span><span class="n">w_cores</span><span class="p">,</span>
                      <span class="n">worker_memory</span><span class="o">=</span><span class="n">w_mem</span><span class="p">,</span>
                      <span class="n">scheduler_vcores</span><span class="o">=</span><span class="n">s_cores</span><span class="p">,</span>
                      <span class="n">scheduler_memory</span><span class="o">=</span><span class="n">s_mem</span><span class="p">,</span>
                      <span class="n">name</span><span class="o">=</span><span class="n">app_name</span><span class="p">,</span>
                      <span class="n">worker_env</span><span class="o">=</span><span class="p">{</span><span class="s1">'ARROW_LIBHDFS_DIR'</span><span class="p">:</span> <span class="n">libhdfs_env</span><span class="p">})</span>
<span class="c1"># Scale out to ten such workers</span>
<span class="n">cluster</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># Connect to the cluster</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="n">cluster</span><span class="p">)</span>
</pre></div>
<p>基本的连接过程如上所示，你需要安装 <code>dask-yarn</code> 模块，然后连接的时候参数</p>
<ul>
<li>environment 你的分布式应用程序的python环境包，你可以激活虚拟环境下运行 <code>venv-pack</code> 命令来打包之。</li>
<li>worker_vcores 申请分配应用资源worker cpu 核数</li>
<li>worker_memory 申请分配应用资源worker 内存数</li>
<li>scheduler_vcores 申请分配应用资源scheduler cpu 核数</li>
<li>scheduler_memory 申请分配应用资源scheduler 内存数</li>
<li>name 申请分配资源的应用名字</li>
<li>work_env 这里是大数据组配置好了hadoop的环境变量</li>
</ul>
<h3 id="worker">实际调整worker资源</h3>
<p>最开始是没有实际开启worker资源的。</p>
<div class="highlight"><pre><span></span><span class="k">cluster</span><span class="p">.</span><span class="k">scale</span><span class="p">()</span>
</pre></div>
<p>然后可以跟个数字，其会自动决定是 <code>scale_up</code> 还是 <code>scale_down</code> 。</p>
<h3 id="_2">关闭应用释放资源</h3>
<p>似乎cluster的关闭动作会申请自动释放你在hadoop集群上申请的资源，但任何非正常推荐都可能导致申请的应用资源仍占在哪里的。更保险起见是确保异常之后总能够执行:</p>
<div class="highlight"><pre><span></span><span class="k">cluster</span><span class="p">.</span><span class="n">shutdown</span><span class="p">()</span>
</pre></div>
<h3 id="_3">强制关闭某个应用</h3>
<div class="highlight"><pre><span></span> <span class="n">dask</span><span class="o">-</span><span class="n">yarn</span> <span class="n">kill</span> <span class="n">application_1538148161343_0051</span>
</pre></div>
<h2 id="dataframe">dataframe最佳实践</h2>
<ol>
<li>如果你的机器内存够用，那么应该直接使用pandas。</li>
<li>代码在运算过程中，如果能够转到pandas，那么也应该尽早转到pandas。</li>
<li>pandas的优化：避免apply，使用向量化操作，用分类等。这些对dask的dataframe都有效。尤其是向量化的操作风格，这个初学pandas的人很容易忽视。</li>
<li>将某一列设置为索引index，某些沿着这个列的操作将会加速，比如说沿着这个列的loc，groupby，join or merge。</li>
<li>上面提到的 <code>set_index</code> 是很昂贵的操作，你需要确认这有必要，此外不要频繁这样设置索引操作，最好后面再persist 一下。</li>
<li>聪明的persist 推荐采用如下写法来节省占用内存：</li>
</ol>
<div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="n">persist</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>  <span class="o">#</span> <span class="k">replace</span> <span class="n">your</span> <span class="k">old</span> <span class="n">lazy</span> <span class="n">DataFrame</span>
</pre></div>
<ol>
<li>设置合理的分区数 分区数太大太小都不好 </li>
<li>数据存为apache parquet文件格式</li>
</ol>
<h2 id="dataframemap_partitions">dataframe的map_partitions 返回值</h2>
<p>参考了 <a href="https://stackoverflow.com/questions/40662912/python-dask-dataframe-map-partitions-return-value">这个网页</a> 。</p>
<ol>
<li>如果函数返回的标量，那么map_partitions 返回的是 dask Series object</li>
<li>如果函数返回的是pd.Series object， 那么 map_partitions 会将这些 Series 对象连接起来，返回的也是 dask Series object。</li>
<li>如果函数返回的pd.DataFrame object ，那么map_partitions 会将这些Dataframe连接起来，沿着axis=0，也就是纵向，返回的是dask DataFrame对象。</li>
</ol>
<h2 id="dask_1">dask单机版</h2>
<p>dask单机版在熟悉dask命令和基本调试代码上还是很有用的，不需要做任何配置就是dask单机版。</p>
<p>按照官方文档，单机版也分为单机调度或分布式调度两种，官方文档推荐如下采用分布式调度，说有更好的诊断功能等。</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">dask.distributed</span> <span class="kn">import</span> <span class="n">Client</span><span class="p">,</span> <span class="n">LocalCluster</span>
<span class="n">cluster</span> <span class="o">=</span> <span class="n">LocalCluster</span><span class="p">()</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="n">cluster</span><span class="p">)</span>
</pre></div>
<p>似乎现在只能在linux上运行了。</p>
<p>单机版的线程调度中还是有所区别的：</p>
<ul>
<li>相同的进程中用多个线程 【】 默认是dask.array dask.dataframe dask.delayed</li>
</ul>
<p>scheduler="threads"</p>
<ul>
<li>分开的进程处理 默认是dask.bag</li>
</ul>
<p>scheduler="processes"</p>
<ul>
<li>单线程式</li>
</ul>
<p>scheduler="single-threaded"</p>
<h2 id="_4">参考资料</h2>
<ol>
<li><a href="https://docs.dask.org/en/latest/">dask官方文档</a></li>
</ol>
            
            
            <hr/>

        </div>
        <section>
        <div class="col-md-2" style="float:right;font-size:0.9em;">
            <h4>首发于：</h4>
            <time pubdate="pubdate" datetime="2019-05-06T00:00:00+08:00">2019年 5月 6日 </time>

<h4>最近更新于：</h4>
<time datetime="2019-05-06T00:00:00+08:00">2019年 5月 6日 </time>

            <h4>分类：</h4>
            <a class="category-link" href="https://docs.cdwanze.work/categories.html#draft-ref">draft</a>
            <h4>标签：</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="https://docs.cdwanze.work/tags.html#dask-ref">dask
</a></li>
            </ul>
        </div>
        </section>
</div>
</article>
    </div>
    <div class="col-md-1"></div>

</div>


<div id="push"></div>


<footer>
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-power">Powered by <a href="http://getpelican.com/" title="Pelican Home Page">Pelican</a>. Theme: <a href="http://oncrashreboot.com/pelican-elegant" title="Theme Elegant Home Page">Elegant</a> by <a href="http://oncrashreboot.com" title="Talha Mansoor Home Page">Talha Mansoor</a> and updated by <a href="http://www.cdwanze.work" title="cdwanze Home Page">cdwanze</a></li>
    </ul>
</div>
</footer>

        <script src="https://docs.cdwanze.work/theme/js/jquery.min.js"></script>
    <script src="https://docs.cdwanze.work/theme/js/bootstrap.min.js"></script>
    <script>
        function validateForm(query) {
            return (query.length > 0);
        }
    </script>

    <script>
        function adjust_search_width() {
            var w = document.documentElement.clientWidth;
            if ((w > 755) && (w < 975)) {
                plus_width = w - 755;
                $('.navbar-form .form-control').outerWidth(210 + plus_width);
            } else if (w >= 975) {
                $('.navbar-form .form-control').outerWidth(210 + 220);
            } else if (w <= 755) {
                $('.navbar-form .form-control').css('width', '100%')
            }
        }

        $(document).ready(function () {
            adjust_search_width();
        });

        window.onresize = function () {
            adjust_search_width();
        }

    </script>


    



</body>
</html>